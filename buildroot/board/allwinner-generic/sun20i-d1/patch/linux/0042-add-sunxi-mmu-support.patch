From 1957805493d82ba36d41c93bd0fc29f8da3fabcf Mon Sep 17 00:00:00 2001
From: YuzukiTsuru <gloomyghost@gloomyghost.com>
Date: Fri, 25 Mar 2022 17:01:12 +0800
Subject: [PATCH 42/93] add sunxi mmu support

---
 drivers/iommu/Kconfig              |   82 +-
 drivers/iommu/Makefile             |    5 +-
 drivers/iommu/arm-smmu-impl.c      |    5 +-
 drivers/iommu/arm-smmu-qcom.c      |   86 ++
 drivers/iommu/arm-smmu-v3.c        |   92 +-
 drivers/iommu/arm-smmu.c           |  196 ++-
 drivers/iommu/arm-smmu.h           |   18 +
 drivers/iommu/dma-iommu.c          |   46 +
 drivers/iommu/io-pgtable-arm-v7s.c |   53 +-
 drivers/iommu/io-pgtable-arm.c     |   37 +-
 drivers/iommu/io-pgtable.c         |   33 +
 drivers/iommu/iommu-sysfs.c        |    5 +
 drivers/iommu/iommu.c              |   42 +-
 drivers/iommu/iova.c               |   94 +-
 drivers/iommu/of_iommu.c           |   17 +-
 drivers/iommu/sunxi-iommu-debug.c  |  952 ++++++++++++++
 drivers/iommu/sunxi-iommu.c        | 1860 ++++++++++++++++++++++++++++
 drivers/iommu/sunxi-iommu.h        |  476 +++++++
 18 files changed, 3951 insertions(+), 148 deletions(-)
 create mode 100644 drivers/iommu/arm-smmu-qcom.c
 create mode 100644 drivers/iommu/sunxi-iommu-debug.c
 create mode 100644 drivers/iommu/sunxi-iommu.c
 create mode 100644 drivers/iommu/sunxi-iommu.h

diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig
index 390568afe..c1d110390 100644
--- a/drivers/iommu/Kconfig
+++ b/drivers/iommu/Kconfig
@@ -7,6 +7,36 @@ config IOMMU_IOVA
 config IOMMU_API
 	bool
 
+if IOMMU_IOVA
+
+config IOMMU_LIMIT_IOVA_ALIGNMENT
+	bool "Limit IOVA alignment"
+	help
+	  When the IOVA framework applies IOVA alignment it aligns all
+	  IOVAs to the smallest PAGE_SIZE order which is greater than or
+	  equal to the requested IOVA size. This works fine for sizes up
+	  to several MiB, but for larger sizes it results in address
+	  space wastage and fragmentation. For example drivers with a 4
+	  GiB IOVA space might run out of IOVA space when allocating
+	  buffers great than 64 MiB.
+
+	  Enable this option to impose a limit on the alignment of IOVAs.
+
+	  If unsure, say N.
+
+config IOMMU_IOVA_ALIGNMENT
+	int "Maximum PAGE_SIZE order of alignment for IOVAs"
+	depends on IOMMU_LIMIT_IOVA_ALIGNMENT
+	range 4 9
+	default 9
+	help
+	  With this parameter you can specify the maximum PAGE_SIZE order for
+	  IOVAs. Larger IOVAs will be aligned only to this specified order.
+	  The order is expressed a power of two multiplied by the PAGE_SIZE.
+
+	  If unsure, leave the default value "9".
+endif
+
 menuconfig IOMMU_SUPPORT
 	bool "IOMMU Hardware Support"
 	depends on MMU
@@ -350,8 +380,9 @@ config SPAPR_TCE_IOMMU
 
 # ARM IOMMU support
 config ARM_SMMU
-	bool "ARM Ltd. System MMU (SMMU) Support"
+	tristate "ARM Ltd. System MMU (SMMU) Support"
 	depends on (ARM64 || ARM) && MMU
+	depends on QCOM_SCM || !QCOM_SCM #if QCOM_SCM=m this can't be =y
 	select IOMMU_API
 	select IOMMU_IO_PGTABLE_LPAE
 	select ARM_DMA_USE_IOMMU if ARM
@@ -362,6 +393,18 @@ config ARM_SMMU
 	  Say Y here if your SoC includes an IOMMU device implementing
 	  the ARM SMMU architecture.
 
+config ARM_SMMU_LEGACY_DT_BINDINGS
+	bool "Support the legacy \"mmu-masters\" devicetree bindings"
+	depends on ARM_SMMU=y && OF
+	help
+	  Support for the badly designed and deprecated "mmu-masters"
+	  devicetree bindings. This allows some DMA masters to attach
+	  to the SMMU but does not provide any support via the DMA API.
+	  If you're lucky, you might be able to get VFIO up and running.
+
+	  If you say Y here then you'll make me very sad. Instead, say N
+	  and move your firmware to the utopian future that was 2016.
+
 config ARM_SMMU_DISABLE_BYPASS_BY_DEFAULT
 	bool "Default to disabling bypass on ARM SMMU v1 and v2"
 	depends on ARM_SMMU
@@ -388,7 +431,7 @@ config ARM_SMMU_DISABLE_BYPASS_BY_DEFAULT
 	  config.
 
 config ARM_SMMU_V3
-	bool "ARM Ltd. System MMU Version 3 (SMMUv3) Support"
+	tristate "ARM Ltd. System MMU Version 3 (SMMUv3) Support"
 	depends on ARM64
 	select IOMMU_API
 	select IOMMU_IO_PGTABLE_LPAE
@@ -459,6 +502,7 @@ config QCOM_IOMMU
 	# Note: iommu drivers cannot (yet?) be built as modules
 	bool "Qualcomm IOMMU Support"
 	depends on ARCH_QCOM || (COMPILE_TEST && !GENERIC_ATOMIC64)
+	depends on QCOM_SCM=y
 	select IOMMU_API
 	select IOMMU_IO_PGTABLE_LPAE
 	select ARM_DMA_USE_IOMMU
@@ -485,4 +529,38 @@ config VIRTIO_IOMMU
 
 	  Say Y here if you intend to run this kernel as a guest.
 
+config SUNXI_IOMMU
+	tristate "Allwinner Sunxi IOMMU Support"
+	depends on ARCH_SUNXI
+	select ARM_DMA_USE_IOMMU if ARM
+	select IOMMU_API
+	select IOMMU_DMA
+	help
+	  Support for the Allwinner's IOMMU(System MMU) component. This
+	  enables H/W multimedia accellerators to see non-linear physical
+	  memory chunks as a linear memory in their address spaces.
+
+	  If unsure,say N here.
+
+menuconfig SUNXI_IOMMU_DEBUG
+	tristate "Sunxi IOMMU Profiling and Debugging"
+	help
+	  Makes available some additional IOMMU profiling and debugging
+	  options.
+
+	  Say Y here if you want to debug and trace iommu driver.
+
+if SUNXI_IOMMU_DEBUG
+
+config SUNXI_IOMMU_TESTS
+	tristate "Sunxi Interactive IOMMU performance/functional tests"
+	depends on SUNXI_IOMMU_DEBUG
+	select IOMMU_API
+	help
+	  Enables a suite of IOMMU unit tests.  The tests are runnable
+	  through debugfs. The impact of enabling this option to overal
+	  system performance should be minimal.
+
+endif # SUNXI_IOMMU_DEBUG
+
 endif # IOMMU_SUPPORT
diff --git a/drivers/iommu/Makefile b/drivers/iommu/Makefile
index 4f405f926..22f293658 100644
--- a/drivers/iommu/Makefile
+++ b/drivers/iommu/Makefile
@@ -13,7 +13,8 @@ obj-$(CONFIG_MSM_IOMMU) += msm_iommu.o
 obj-$(CONFIG_AMD_IOMMU) += amd_iommu.o amd_iommu_init.o amd_iommu_quirks.o
 obj-$(CONFIG_AMD_IOMMU_DEBUGFS) += amd_iommu_debugfs.o
 obj-$(CONFIG_AMD_IOMMU_V2) += amd_iommu_v2.o
-obj-$(CONFIG_ARM_SMMU) += arm-smmu.o arm-smmu-impl.o
+obj-$(CONFIG_ARM_SMMU) += arm_smmu.o
+arm_smmu-objs += arm-smmu.o arm-smmu-impl.o arm-smmu-qcom.o
 obj-$(CONFIG_ARM_SMMU_V3) += arm-smmu-v3.o
 obj-$(CONFIG_DMAR_TABLE) += dmar.o
 obj-$(CONFIG_INTEL_IOMMU) += intel-iommu.o intel-pasid.o
@@ -35,3 +36,5 @@ obj-$(CONFIG_S390_IOMMU) += s390-iommu.o
 obj-$(CONFIG_QCOM_IOMMU) += qcom_iommu.o
 obj-$(CONFIG_HYPERV_IOMMU) += hyperv-iommu.o
 obj-$(CONFIG_VIRTIO_IOMMU) += virtio-iommu.o
+obj-$(CONFIG_SUNXI_IOMMU) += sunxi-iommu.o
+obj-$(CONFIG_SUNXI_IOMMU_DEBUG) += sunxi-iommu-debug.o
diff --git a/drivers/iommu/arm-smmu-impl.c b/drivers/iommu/arm-smmu-impl.c
index 5c87a3862..b2fe72a8f 100644
--- a/drivers/iommu/arm-smmu-impl.c
+++ b/drivers/iommu/arm-smmu-impl.c
@@ -109,7 +109,7 @@ static struct arm_smmu_device *cavium_smmu_impl_init(struct arm_smmu_device *smm
 #define ARM_MMU500_ACR_S2CRB_TLBEN	(1 << 10)
 #define ARM_MMU500_ACR_SMTNMB_TLBEN	(1 << 8)
 
-static int arm_mmu500_reset(struct arm_smmu_device *smmu)
+int arm_mmu500_reset(struct arm_smmu_device *smmu)
 {
 	u32 reg, major;
 	int i;
@@ -170,5 +170,8 @@ struct arm_smmu_device *arm_smmu_impl_init(struct arm_smmu_device *smmu)
 				  "calxeda,smmu-secure-config-access"))
 		smmu->impl = &calxeda_impl;
 
+	if (of_device_is_compatible(smmu->dev->of_node, "qcom,sdm845-smmu-500"))
+		return qcom_smmu_impl_init(smmu);
+
 	return smmu;
 }
diff --git a/drivers/iommu/arm-smmu-qcom.c b/drivers/iommu/arm-smmu-qcom.c
new file mode 100644
index 000000000..06e5799dc
--- /dev/null
+++ b/drivers/iommu/arm-smmu-qcom.c
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2019, The Linux Foundation. All rights reserved.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/qcom_scm.h>
+
+#include "arm-smmu.h"
+
+struct qcom_smmu {
+	struct arm_smmu_device smmu;
+};
+
+static int qcom_sdm845_smmu500_cfg_probe(struct arm_smmu_device *smmu)
+{
+	u32 s2cr;
+	u32 smr;
+	int i;
+
+	for (i = 0; i < smmu->num_mapping_groups; i++) {
+		smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(i));
+		s2cr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_S2CR(i));
+
+		smmu->smrs[i].mask = FIELD_GET(SMR_MASK, smr);
+		smmu->smrs[i].id = FIELD_GET(SMR_ID, smr);
+		if (smmu->features & ARM_SMMU_FEAT_EXIDS)
+			smmu->smrs[i].valid = FIELD_GET(S2CR_EXIDVALID, s2cr);
+		else
+			smmu->smrs[i].valid = FIELD_GET(SMR_VALID, smr);
+
+		smmu->s2crs[i].group = NULL;
+		smmu->s2crs[i].count = 0;
+		smmu->s2crs[i].type = FIELD_GET(S2CR_TYPE, s2cr);
+		smmu->s2crs[i].privcfg = FIELD_GET(S2CR_PRIVCFG, s2cr);
+		smmu->s2crs[i].cbndx = FIELD_GET(S2CR_CBNDX, s2cr);
+
+		if (!smmu->smrs[i].valid)
+			continue;
+
+		smmu->s2crs[i].pinned = true;
+		bitmap_set(smmu->context_map, smmu->s2crs[i].cbndx, 1);
+	}
+
+	return 0;
+}
+
+static int qcom_sdm845_smmu500_reset(struct arm_smmu_device *smmu)
+{
+	int ret;
+
+	arm_mmu500_reset(smmu);
+
+	/*
+	 * To address performance degradation in non-real time clients,
+	 * such as USB and UFS, turn off wait-for-safe on sdm845 based boards,
+	 * such as MTP and db845, whose firmwares implement secure monitor
+	 * call handlers to turn on/off the wait-for-safe logic.
+	 */
+	ret = qcom_scm_qsmmu500_wait_safe_toggle(0);
+	if (ret)
+		dev_warn(smmu->dev, "Failed to turn off SAFE logic\n");
+
+	return ret;
+}
+
+static const struct arm_smmu_impl qcom_smmu_impl = {
+	.cfg_probe = qcom_sdm845_smmu500_cfg_probe,
+	.reset = qcom_sdm845_smmu500_reset,
+};
+
+struct arm_smmu_device *qcom_smmu_impl_init(struct arm_smmu_device *smmu)
+{
+	struct qcom_smmu *qsmmu;
+
+	qsmmu = devm_kzalloc(smmu->dev, sizeof(*qsmmu), GFP_KERNEL);
+	if (!qsmmu)
+		return ERR_PTR(-ENOMEM);
+
+	qsmmu->smmu = *smmu;
+
+	qsmmu->smmu.impl = &qcom_smmu_impl;
+	devm_kfree(smmu->dev, smmu);
+
+	return &qsmmu->smmu;
+}
diff --git a/drivers/iommu/arm-smmu-v3.c b/drivers/iommu/arm-smmu-v3.c
index ef6af714a..2802ca291 100644
--- a/drivers/iommu/arm-smmu-v3.c
+++ b/drivers/iommu/arm-smmu-v3.c
@@ -21,8 +21,7 @@
 #include <linux/io-pgtable.h>
 #include <linux/iommu.h>
 #include <linux/iopoll.h>
-#include <linux/init.h>
-#include <linux/moduleparam.h>
+#include <linux/module.h>
 #include <linux/msi.h>
 #include <linux/of.h>
 #include <linux/of_address.h>
@@ -384,10 +383,6 @@
 #define MSI_IOVA_BASE			0x8000000
 #define MSI_IOVA_LENGTH			0x100000
 
-/*
- * not really modular, but the easiest way to keep compat with existing
- * bootargs behaviour is to continue using module_param_named here.
- */
 static bool disable_bypass = 1;
 module_param_named(disable_bypass, disable_bypass, bool, S_IRUGO);
 MODULE_PARM_DESC(disable_bypass,
@@ -3572,6 +3567,43 @@ static unsigned long arm_smmu_resource_size(struct arm_smmu_device *smmu)
 		return SZ_128K;
 }
 
+static int arm_smmu_set_bus_ops(struct iommu_ops *ops)
+{
+	int err;
+
+#ifdef CONFIG_PCI
+	if (pci_bus_type.iommu_ops != ops) {
+		err = bus_set_iommu(&pci_bus_type, ops);
+		if (err)
+			return err;
+	}
+#endif
+#ifdef CONFIG_ARM_AMBA
+	if (amba_bustype.iommu_ops != ops) {
+		err = bus_set_iommu(&amba_bustype, ops);
+		if (err)
+			goto err_reset_pci_ops;
+	}
+#endif
+	if (platform_bus_type.iommu_ops != ops) {
+		err = bus_set_iommu(&platform_bus_type, ops);
+		if (err)
+			goto err_reset_amba_ops;
+	}
+
+	return 0;
+
+err_reset_amba_ops:
+#ifdef CONFIG_ARM_AMBA
+	bus_set_iommu(&amba_bustype, NULL);
+#endif
+err_reset_pci_ops: __maybe_unused;
+#ifdef CONFIG_PCI
+	bus_set_iommu(&pci_bus_type, NULL);
+#endif
+	return err;
+}
+
 static int arm_smmu_device_probe(struct platform_device *pdev)
 {
 	int irq, ret;
@@ -3662,48 +3694,44 @@ static int arm_smmu_device_probe(struct platform_device *pdev)
 		return ret;
 	}
 
-#ifdef CONFIG_PCI
-	if (pci_bus_type.iommu_ops != &arm_smmu_ops) {
-		pci_request_acs();
-		ret = bus_set_iommu(&pci_bus_type, &arm_smmu_ops);
-		if (ret)
-			return ret;
-	}
-#endif
-#ifdef CONFIG_ARM_AMBA
-	if (amba_bustype.iommu_ops != &arm_smmu_ops) {
-		ret = bus_set_iommu(&amba_bustype, &arm_smmu_ops);
-		if (ret)
-			return ret;
-	}
-#endif
-	if (platform_bus_type.iommu_ops != &arm_smmu_ops) {
-		ret = bus_set_iommu(&platform_bus_type, &arm_smmu_ops);
-		if (ret)
-			return ret;
-	}
-	return 0;
+	return arm_smmu_set_bus_ops(&arm_smmu_ops);
 }
 
-static void arm_smmu_device_shutdown(struct platform_device *pdev)
+static int arm_smmu_device_remove(struct platform_device *pdev)
 {
 	struct arm_smmu_device *smmu = platform_get_drvdata(pdev);
 
+	arm_smmu_set_bus_ops(NULL);
+	iommu_device_unregister(&smmu->iommu);
+	iommu_device_sysfs_remove(&smmu->iommu);
 	arm_smmu_device_disable(smmu);
+
+	return 0;
+}
+
+static void arm_smmu_device_shutdown(struct platform_device *pdev)
+{
+	arm_smmu_device_remove(pdev);
 }
 
 static const struct of_device_id arm_smmu_of_match[] = {
 	{ .compatible = "arm,smmu-v3", },
 	{ },
 };
+MODULE_DEVICE_TABLE(of, arm_smmu_of_match);
 
 static struct platform_driver arm_smmu_driver = {
 	.driver	= {
-		.name		= "arm-smmu-v3",
-		.of_match_table	= of_match_ptr(arm_smmu_of_match),
-		.suppress_bind_attrs = true,
+		.name			= "arm-smmu-v3",
+		.of_match_table		= of_match_ptr(arm_smmu_of_match),
+		.suppress_bind_attrs	= true,
 	},
 	.probe	= arm_smmu_device_probe,
+	.remove	= arm_smmu_device_remove,
 	.shutdown = arm_smmu_device_shutdown,
 };
-builtin_platform_driver(arm_smmu_driver);
+module_platform_driver(arm_smmu_driver);
+
+MODULE_DESCRIPTION("IOMMU API for ARM architected SMMUv3 implementations");
+MODULE_AUTHOR("Will Deacon <will@kernel.org>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/iommu/arm-smmu.c b/drivers/iommu/arm-smmu.c
index 7c503a6bc..3c32958ff 100644
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@ -27,8 +27,7 @@
 #include <linux/interrupt.h>
 #include <linux/io.h>
 #include <linux/iopoll.h>
-#include <linux/init.h>
-#include <linux/moduleparam.h>
+#include <linux/module.h>
 #include <linux/of.h>
 #include <linux/of_address.h>
 #include <linux/of_device.h>
@@ -59,10 +58,6 @@
 #define MSI_IOVA_LENGTH			0x100000
 
 static int force_stage;
-/*
- * not really modular, but the easiest way to keep compat with existing
- * bootargs behaviour is to continue using module_param() here.
- */
 module_param(force_stage, int, S_IRUGO);
 MODULE_PARM_DESC(force_stage,
 	"Force SMMU mappings to be installed at a particular stage of translation. A value of '1' or '2' forces the corresponding stage. All other values are ignored (i.e. no stage is forced). Note that selecting a specific stage will disable support for nested translation.");
@@ -72,24 +67,10 @@ module_param(disable_bypass, bool, S_IRUGO);
 MODULE_PARM_DESC(disable_bypass,
 	"Disable bypass streams such that incoming transactions from devices that are not attached to an iommu domain will report an abort back to the device and will not be allowed to pass through the SMMU.");
 
-struct arm_smmu_s2cr {
-	struct iommu_group		*group;
-	int				count;
-	enum arm_smmu_s2cr_type		type;
-	enum arm_smmu_s2cr_privcfg	privcfg;
-	u8				cbndx;
-};
-
 #define s2cr_init_val (struct arm_smmu_s2cr){				\
 	.type = disable_bypass ? S2CR_TYPE_FAULT : S2CR_TYPE_BYPASS,	\
 }
 
-struct arm_smmu_smr {
-	u16				mask;
-	u16				id;
-	bool				valid;
-};
-
 struct arm_smmu_cb {
 	u64				ttbr[2];
 	u32				tcr[2];
@@ -130,6 +111,12 @@ static struct arm_smmu_domain *to_smmu_domain(struct iommu_domain *dom)
 	return container_of(dom, struct arm_smmu_domain, domain);
 }
 
+static struct platform_driver arm_smmu_driver;
+static struct iommu_ops arm_smmu_ops;
+
+#ifdef CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS
+static int arm_smmu_bus_init(struct iommu_ops *ops);
+
 static struct device_node *dev_get_dev_node(struct device *dev)
 {
 	if (dev_is_pci(dev)) {
@@ -165,9 +152,6 @@ static int __find_legacy_master_phandle(struct device *dev, void *data)
 	return err == -ENOENT ? 0 : err;
 }
 
-static struct platform_driver arm_smmu_driver;
-static struct iommu_ops arm_smmu_ops;
-
 static int arm_smmu_register_legacy_master(struct device *dev,
 					   struct arm_smmu_device **smmu)
 {
@@ -219,9 +203,40 @@ static int arm_smmu_register_legacy_master(struct device *dev,
 	return err;
 }
 
-static int __arm_smmu_alloc_bitmap(unsigned long *map, int start, int end)
+/*
+ * With the legacy DT binding in play, we have no guarantees about
+ * probe order, but then we're also not doing default domains, so we can
+ * delay setting bus ops until we're sure every possible SMMU is ready,
+ * and that way ensure that no add_device() calls get missed.
+ */
+static int arm_smmu_legacy_bus_init(void)
+{
+	if (using_legacy_binding)
+		return arm_smmu_bus_init(&arm_smmu_ops);
+	return 0;
+}
+device_initcall_sync(arm_smmu_legacy_bus_init);
+#else
+static int arm_smmu_register_legacy_master(struct device *dev,
+					   struct arm_smmu_device **smmu)
+{
+	return -ENODEV;
+}
+#endif /* CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS */
+
+static int __arm_smmu_alloc_cb(struct arm_smmu_device *smmu, int start,
+			       struct device *dev)
 {
+	struct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);
+	unsigned long *map = smmu->context_map;
+	int end = smmu->num_context_banks;
 	int idx;
+	int i;
+
+	for_each_cfg_sme(fwspec, i, idx) {
+		if (smmu->s2crs[idx].pinned)
+			return smmu->s2crs[idx].cbndx;
+	}
 
 	do {
 		idx = find_next_zero_bit(map, end, start);
@@ -626,7 +641,8 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 }
 
 static int arm_smmu_init_domain_context(struct iommu_domain *domain,
-					struct arm_smmu_device *smmu)
+					struct arm_smmu_device *smmu,
+					struct device *dev)
 {
 	int irq, start, ret = 0;
 	unsigned long ias, oas;
@@ -740,8 +756,7 @@ static int arm_smmu_init_domain_context(struct iommu_domain *domain,
 		ret = -EINVAL;
 		goto out_unlock;
 	}
-	ret = __arm_smmu_alloc_bitmap(smmu->context_map, start,
-				      smmu->num_context_banks);
+	ret = __arm_smmu_alloc_cb(smmu, start, dev);
 	if (ret < 0)
 		goto out_unlock;
 
@@ -929,24 +944,43 @@ static void arm_smmu_write_sme(struct arm_smmu_device *smmu, int idx)
  */
 static void arm_smmu_test_smr_masks(struct arm_smmu_device *smmu)
 {
+	u32 s2cr;
 	u32 smr;
+	int idx;
 
 	if (!smmu->smrs)
 		return;
 
+	for (idx = 0; idx < smmu->num_mapping_groups; idx++) {
+		if (smmu->features & ARM_SMMU_FEAT_EXIDS) {
+			s2cr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_S2CR(idx));
+			if (!FIELD_GET(S2CR_EXIDVALID, s2cr))
+				break;
+		} else {
+			smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(idx));
+			if (!FIELD_GET(SMR_VALID, smr))
+				break;
+		}
+	}
+
+	if (idx == smmu->num_mapping_groups) {
+		dev_err(smmu->dev, "Unable to compute streamid_mask\n");
+		return;
+	}
+
 	/*
 	 * SMR.ID bits may not be preserved if the corresponding MASK
 	 * bits are set, so check each one separately. We can reject
 	 * masters later if they try to claim IDs outside these masks.
 	 */
 	smr = FIELD_PREP(SMR_ID, smmu->streamid_mask);
-	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(0), smr);
-	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(0));
+	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(idx), smr);
+	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(idx));
 	smmu->streamid_mask = FIELD_GET(SMR_ID, smr);
 
 	smr = FIELD_PREP(SMR_MASK, smmu->streamid_mask);
-	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(0), smr);
-	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(0));
+	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(idx), smr);
+	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(idx));
 	smmu->smr_mask_mask = FIELD_GET(SMR_MASK, smr);
 }
 
@@ -994,12 +1028,19 @@ static int arm_smmu_find_sme(struct arm_smmu_device *smmu, u16 id, u16 mask)
 
 static bool arm_smmu_free_sme(struct arm_smmu_device *smmu, int idx)
 {
+	bool pinned = smmu->s2crs[idx].pinned;
+	u8 cbndx = smmu->s2crs[idx].cbndx;;
+
 	if (--smmu->s2crs[idx].count)
 		return false;
 
 	smmu->s2crs[idx] = s2cr_init_val;
-	if (smmu->smrs)
+	if (pinned) {
+		smmu->s2crs[idx].pinned = true;
+		smmu->s2crs[idx].cbndx = cbndx;
+	} else if (smmu->smrs) {
 		smmu->smrs[idx].valid = false;
+	}
 
 	return true;
 }
@@ -1135,7 +1176,7 @@ static int arm_smmu_attach_dev(struct iommu_domain *domain, struct device *dev)
 		return ret;
 
 	/* Ensure that the domain is finalised */
-	ret = arm_smmu_init_domain_context(domain, smmu);
+	ret = arm_smmu_init_domain_context(domain, smmu, dev);
 	if (ret < 0)
 		goto rpm_put;
 
@@ -1878,6 +1919,7 @@ static const struct of_device_id arm_smmu_of_match[] = {
 	{ .compatible = "qcom,smmu-v2", .data = &qcom_smmuv2 },
 	{ },
 };
+MODULE_DEVICE_TABLE(of, arm_smmu_of_match);
 
 #ifdef CONFIG_ACPI
 static int acpi_smmu_get_data(u32 model, struct arm_smmu_device *smmu)
@@ -1964,8 +2006,10 @@ static int arm_smmu_device_dt_probe(struct platform_device *pdev,
 
 	legacy_binding = of_find_property(dev->of_node, "mmu-masters", NULL);
 	if (legacy_binding && !using_generic_binding) {
-		if (!using_legacy_binding)
-			pr_notice("deprecated \"mmu-masters\" DT property in use; DMA API support unavailable\n");
+		if (!using_legacy_binding) {
+			pr_notice("deprecated \"mmu-masters\" DT property in use; %s support unavailable\n",
+				  IS_ENABLED(CONFIG_ARM_SMMU_LEGACY_DT_BINDINGS) ? "DMA API" : "SMMU");
+		}
 		using_legacy_binding = true;
 	} else if (!legacy_binding && !using_legacy_binding) {
 		using_generic_binding = true;
@@ -1980,25 +2024,50 @@ static int arm_smmu_device_dt_probe(struct platform_device *pdev,
 	return 0;
 }
 
-static void arm_smmu_bus_init(void)
+static int arm_smmu_bus_init(struct iommu_ops *ops)
 {
+	int err;
+
 	/* Oh, for a proper bus abstraction */
-	if (!iommu_present(&platform_bus_type))
-		bus_set_iommu(&platform_bus_type, &arm_smmu_ops);
+	if (!iommu_present(&platform_bus_type)) {
+		err = bus_set_iommu(&platform_bus_type, ops);
+		if (err)
+			return err;
+	}
 #ifdef CONFIG_ARM_AMBA
-	if (!iommu_present(&amba_bustype))
-		bus_set_iommu(&amba_bustype, &arm_smmu_ops);
+	if (!iommu_present(&amba_bustype)) {
+		err = bus_set_iommu(&amba_bustype, ops);
+		if (err)
+			goto err_reset_platform_ops;
+	}
 #endif
 #ifdef CONFIG_PCI
 	if (!iommu_present(&pci_bus_type)) {
-		pci_request_acs();
-		bus_set_iommu(&pci_bus_type, &arm_smmu_ops);
+		err = bus_set_iommu(&pci_bus_type, ops);
+		if (err)
+			goto err_reset_amba_ops;
 	}
 #endif
 #ifdef CONFIG_FSL_MC_BUS
-	if (!iommu_present(&fsl_mc_bus_type))
-		bus_set_iommu(&fsl_mc_bus_type, &arm_smmu_ops);
+	if (!iommu_present(&fsl_mc_bus_type)) {
+		err = bus_set_iommu(&fsl_mc_bus_type, ops);
+		if (err)
+			goto err_reset_pci_ops;
+	}
+#endif
+	return 0;
+
+err_reset_pci_ops: __maybe_unused;
+#ifdef CONFIG_PCI
+	bus_set_iommu(&pci_bus_type, NULL);
 #endif
+err_reset_amba_ops: __maybe_unused;
+#ifdef CONFIG_ARM_AMBA
+	bus_set_iommu(&amba_bustype, NULL);
+#endif
+err_reset_platform_ops: __maybe_unused;
+	bus_set_iommu(&platform_bus_type, NULL);
+	return err;
 }
 
 static int arm_smmu_device_probe(struct platform_device *pdev)
@@ -2146,35 +2215,25 @@ static int arm_smmu_device_probe(struct platform_device *pdev)
 	 * ready to handle default domain setup as soon as any SMMU exists.
 	 */
 	if (!using_legacy_binding)
-		arm_smmu_bus_init();
+		return arm_smmu_bus_init(&arm_smmu_ops);
 
 	return 0;
 }
 
-/*
- * With the legacy DT binding in play, though, we have no guarantees about
- * probe order, but then we're also not doing default domains, so we can
- * delay setting bus ops until we're sure every possible SMMU is ready,
- * and that way ensure that no add_device() calls get missed.
- */
-static int arm_smmu_legacy_bus_init(void)
-{
-	if (using_legacy_binding)
-		arm_smmu_bus_init();
-	return 0;
-}
-device_initcall_sync(arm_smmu_legacy_bus_init);
-
-static void arm_smmu_device_shutdown(struct platform_device *pdev)
+static int arm_smmu_device_remove(struct platform_device *pdev)
 {
 	struct arm_smmu_device *smmu = platform_get_drvdata(pdev);
 
 	if (!smmu)
-		return;
+		return -ENODEV;
 
 	if (!bitmap_empty(smmu->context_map, ARM_SMMU_MAX_CBS))
 		dev_err(&pdev->dev, "removing device with active domains!\n");
 
+	arm_smmu_bus_init(NULL);
+	iommu_device_unregister(&smmu->iommu);
+	iommu_device_sysfs_remove(&smmu->iommu);
+
 	arm_smmu_rpm_get(smmu);
 	/* Turn the thing off */
 	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_sCR0, sCR0_CLIENTPD);
@@ -2186,6 +2245,12 @@ static void arm_smmu_device_shutdown(struct platform_device *pdev)
 		clk_bulk_disable(smmu->num_clks, smmu->clks);
 
 	clk_bulk_unprepare(smmu->num_clks, smmu->clks);
+	return 0;
+}
+
+static void arm_smmu_device_shutdown(struct platform_device *pdev)
+{
+	arm_smmu_device_remove(pdev);
 }
 
 static int __maybe_unused arm_smmu_runtime_resume(struct device *dev)
@@ -2238,9 +2303,14 @@ static struct platform_driver arm_smmu_driver = {
 		.name			= "arm-smmu",
 		.of_match_table		= of_match_ptr(arm_smmu_of_match),
 		.pm			= &arm_smmu_pm_ops,
-		.suppress_bind_attrs	= true,
+		.suppress_bind_attrs    = true,
 	},
 	.probe	= arm_smmu_device_probe,
+	.remove	= arm_smmu_device_remove,
 	.shutdown = arm_smmu_device_shutdown,
 };
-builtin_platform_driver(arm_smmu_driver);
+module_platform_driver(arm_smmu_driver);
+
+MODULE_DESCRIPTION("IOMMU API for ARM architected SMMU implementations");
+MODULE_AUTHOR("Will Deacon <will@kernel.org>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/iommu/arm-smmu.h b/drivers/iommu/arm-smmu.h
index b19b6cae9..a144c9fa4 100644
--- a/drivers/iommu/arm-smmu.h
+++ b/drivers/iommu/arm-smmu.h
@@ -222,6 +222,21 @@ enum arm_smmu_implementation {
 	QCOM_SMMUV2,
 };
 
+struct arm_smmu_s2cr {
+	struct iommu_group		*group;
+	int				count;
+	enum arm_smmu_s2cr_type		type;
+	enum arm_smmu_s2cr_privcfg	privcfg;
+	u8				cbndx;
+	bool				pinned;
+};
+
+struct arm_smmu_smr {
+	u16				mask;
+	u16				id;
+	bool				valid;
+};
+
 struct arm_smmu_device {
 	struct device			*dev;
 
@@ -398,5 +413,8 @@ static inline void arm_smmu_writeq(struct arm_smmu_device *smmu, int page,
 	arm_smmu_writeq((s), ARM_SMMU_CB((s), (n)), (o), (v))
 
 struct arm_smmu_device *arm_smmu_impl_init(struct arm_smmu_device *smmu);
+struct arm_smmu_device *qcom_smmu_impl_init(struct arm_smmu_device *smmu);
+
+int arm_mmu500_reset(struct arm_smmu_device *smmu);
 
 #endif /* _ARM_SMMU_H */
diff --git a/drivers/iommu/dma-iommu.c b/drivers/iommu/dma-iommu.c
index 76bd2309e..7e6a5cd19 100644
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@ -352,6 +352,52 @@ static int iommu_dma_init_domain(struct iommu_domain *domain, dma_addr_t base,
 	return iova_reserve_iommu_regions(dev, domain);
 }
 
+/*
+ * Should be called prior to using dma-apis
+ */
+int iommu_dma_reserve_iova(struct device *dev, dma_addr_t base,
+			   u64 size)
+{
+	struct iommu_domain *domain;
+	struct iommu_dma_cookie *cookie;
+	struct iova_domain *iovad;
+	unsigned long pfn_lo, pfn_hi;
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain || !domain->iova_cookie)
+		return -EINVAL;
+
+	cookie = domain->iova_cookie;
+	iovad = &cookie->iovad;
+
+	/* iova will be freed automatically by put_iova_domain() */
+	pfn_lo = iova_pfn(iovad, base);
+	pfn_hi = iova_pfn(iovad, base + size - 1);
+	if (!reserve_iova(iovad, pfn_lo, pfn_hi))
+		return -EINVAL;
+
+	return 0;
+}
+EXPORT_SYMBOL(iommu_dma_reserve_iova);
+
+/*
+ * Should be called prior to using dma-apis.
+ */
+int iommu_dma_enable_best_fit_algo(struct device *dev)
+{
+	struct iommu_domain *domain;
+	struct iova_domain *iovad;
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain || !domain->iova_cookie)
+		return -EINVAL;
+
+	iovad = &((struct iommu_dma_cookie *)domain->iova_cookie)->iovad;
+	iovad->best_fit = true;
+	return 0;
+}
+EXPORT_SYMBOL(iommu_dma_enable_best_fit_algo);
+
 /**
  * dma_info_to_prot - Translate DMA API directions and attributes to IOMMU API
  *                    page flags.
diff --git a/drivers/iommu/io-pgtable-arm-v7s.c b/drivers/iommu/io-pgtable-arm-v7s.c
index 4cb394937..6c08e991c 100644
--- a/drivers/iommu/io-pgtable-arm-v7s.c
+++ b/drivers/iommu/io-pgtable-arm-v7s.c
@@ -50,20 +50,26 @@
  */
 #define ARM_V7S_ADDR_BITS		32
 #define _ARM_V7S_LVL_BITS(lvl)		(16 - (lvl) * 4)
+/* MediaTek: totally 34bits, 14bits at lvl1 and 8bits at lvl2. */
+#define _ARM_V7S_LVL_BITS_MTK(lvl)	(20 - (lvl) * 6)
 #define ARM_V7S_LVL_SHIFT(lvl)		(ARM_V7S_ADDR_BITS - (4 + 8 * (lvl)))
 #define ARM_V7S_TABLE_SHIFT		10
 
-#define ARM_V7S_PTES_PER_LVL(lvl)	(1 << _ARM_V7S_LVL_BITS(lvl))
-#define ARM_V7S_TABLE_SIZE(lvl)						\
-	(ARM_V7S_PTES_PER_LVL(lvl) * sizeof(arm_v7s_iopte))
+#define ARM_V7S_PTES_PER_LVL(lvl, cfg)	({				\
+	!arm_v7s_is_mtk_enabled(cfg) ?					\
+	 (1 << _ARM_V7S_LVL_BITS(lvl)) : (1 << _ARM_V7S_LVL_BITS_MTK(lvl));\
+})
+
+#define ARM_V7S_TABLE_SIZE(lvl, cfg)					\
+	(ARM_V7S_PTES_PER_LVL(lvl, cfg) * sizeof(arm_v7s_iopte))
 
 #define ARM_V7S_BLOCK_SIZE(lvl)		(1UL << ARM_V7S_LVL_SHIFT(lvl))
 #define ARM_V7S_LVL_MASK(lvl)		((u32)(~0U << ARM_V7S_LVL_SHIFT(lvl)))
 #define ARM_V7S_TABLE_MASK		((u32)(~0U << ARM_V7S_TABLE_SHIFT))
-#define _ARM_V7S_IDX_MASK(lvl)		(ARM_V7S_PTES_PER_LVL(lvl) - 1)
-#define ARM_V7S_LVL_IDX(addr, lvl)	({				\
+#define _ARM_V7S_IDX_MASK(lvl, cfg)	(ARM_V7S_PTES_PER_LVL(lvl, cfg) - 1)
+#define ARM_V7S_LVL_IDX(addr, lvl, cfg)	({			\
 	int _l = lvl;							\
-	((u32)(addr) >> ARM_V7S_LVL_SHIFT(_l)) & _ARM_V7S_IDX_MASK(_l); \
+	((addr) >> ARM_V7S_LVL_SHIFT(_l)) & _ARM_V7S_IDX_MASK(_l, cfg); \
 })
 
 /*
@@ -112,9 +118,10 @@
 #define ARM_V7S_TEX_MASK		0x7
 #define ARM_V7S_ATTR_TEX(val)		(((val) & ARM_V7S_TEX_MASK) << ARM_V7S_TEX_SHIFT)
 
-/* MediaTek extend the two bits for PA 32bit/33bit */
+/* MediaTek extend the bits below for PA 32bit/33bit/34bit */
 #define ARM_V7S_ATTR_MTK_PA_BIT32	BIT(9)
 #define ARM_V7S_ATTR_MTK_PA_BIT33	BIT(4)
+#define ARM_V7S_ATTR_MTK_PA_BIT34	BIT(5)
 
 /* *well, except for TEX on level 2 large pages, of course :( */
 #define ARM_V7S_CONT_PAGE_TEX_SHIFT	6
@@ -196,6 +203,8 @@ static arm_v7s_iopte paddr_to_iopte(phys_addr_t paddr, int lvl,
 		pte |= ARM_V7S_ATTR_MTK_PA_BIT32;
 	if (paddr & BIT_ULL(33))
 		pte |= ARM_V7S_ATTR_MTK_PA_BIT33;
+	if (paddr & BIT_ULL(34))
+		pte |= ARM_V7S_ATTR_MTK_PA_BIT34;
 	return pte;
 }
 
@@ -220,6 +229,8 @@ static phys_addr_t iopte_to_paddr(arm_v7s_iopte pte, int lvl,
 		paddr |= BIT_ULL(32);
 	if (pte & ARM_V7S_ATTR_MTK_PA_BIT33)
 		paddr |= BIT_ULL(33);
+	if (pte & ARM_V7S_ATTR_MTK_PA_BIT34)
+		paddr |= BIT_ULL(34);
 	return paddr;
 }
 
@@ -236,7 +247,7 @@ static void *__arm_v7s_alloc_table(int lvl, gfp_t gfp,
 	struct device *dev = cfg->iommu_dev;
 	phys_addr_t phys;
 	dma_addr_t dma;
-	size_t size = ARM_V7S_TABLE_SIZE(lvl);
+	size_t size = ARM_V7S_TABLE_SIZE(lvl, cfg);
 	void *table = NULL;
 
 	if (lvl == 1)
@@ -282,7 +293,7 @@ static void __arm_v7s_free_table(void *table, int lvl,
 {
 	struct io_pgtable_cfg *cfg = &data->iop.cfg;
 	struct device *dev = cfg->iommu_dev;
-	size_t size = ARM_V7S_TABLE_SIZE(lvl);
+	size_t size = ARM_V7S_TABLE_SIZE(lvl, cfg);
 
 	if (!cfg->coherent_walk)
 		dma_unmap_single(dev, __arm_v7s_dma_addr(table), size,
@@ -426,7 +437,7 @@ static int arm_v7s_init_pte(struct arm_v7s_io_pgtable *data,
 			arm_v7s_iopte *tblp;
 			size_t sz = ARM_V7S_BLOCK_SIZE(lvl);
 
-			tblp = ptep - ARM_V7S_LVL_IDX(iova, lvl);
+			tblp = ptep - ARM_V7S_LVL_IDX(iova, lvl, cfg);
 			if (WARN_ON(__arm_v7s_unmap(data, NULL, iova + i * sz,
 						    sz, lvl, tblp) != sz))
 				return -EINVAL;
@@ -479,7 +490,7 @@ static int __arm_v7s_map(struct arm_v7s_io_pgtable *data, unsigned long iova,
 	int num_entries = size >> ARM_V7S_LVL_SHIFT(lvl);
 
 	/* Find our entry at the current level */
-	ptep += ARM_V7S_LVL_IDX(iova, lvl);
+	ptep += ARM_V7S_LVL_IDX(iova, lvl, cfg);
 
 	/* If we can install a leaf entry at this level, then do so */
 	if (num_entries)
@@ -552,7 +563,7 @@ static void arm_v7s_free_pgtable(struct io_pgtable *iop)
 	struct arm_v7s_io_pgtable *data = io_pgtable_to_data(iop);
 	int i;
 
-	for (i = 0; i < ARM_V7S_PTES_PER_LVL(1); i++) {
+	for (i = 0; i < ARM_V7S_PTES_PER_LVL(1, &data->iop.cfg); i++) {
 		arm_v7s_iopte pte = data->pgd[i];
 
 		if (ARM_V7S_PTE_IS_TABLE(pte, 1))
@@ -604,9 +615,9 @@ static size_t arm_v7s_split_blk_unmap(struct arm_v7s_io_pgtable *data,
 	if (!tablep)
 		return 0; /* Bytes unmapped */
 
-	num_ptes = ARM_V7S_PTES_PER_LVL(2);
+	num_ptes = ARM_V7S_PTES_PER_LVL(2, cfg);
 	num_entries = size >> ARM_V7S_LVL_SHIFT(2);
-	unmap_idx = ARM_V7S_LVL_IDX(iova, 2);
+	unmap_idx = ARM_V7S_LVL_IDX(iova, 2, cfg);
 
 	pte = arm_v7s_prot_to_pte(arm_v7s_pte_to_prot(blk_pte, 1), 2, cfg);
 	if (num_entries > 1)
@@ -648,7 +659,7 @@ static size_t __arm_v7s_unmap(struct arm_v7s_io_pgtable *data,
 	if (WARN_ON(lvl > 2))
 		return 0;
 
-	idx = ARM_V7S_LVL_IDX(iova, lvl);
+	idx = ARM_V7S_LVL_IDX(iova, lvl, &iop->cfg);
 	ptep += idx;
 	do {
 		pte[i] = READ_ONCE(ptep[i]);
@@ -719,7 +730,7 @@ static size_t arm_v7s_unmap(struct io_pgtable_ops *ops, unsigned long iova,
 {
 	struct arm_v7s_io_pgtable *data = io_pgtable_ops_to_data(ops);
 
-	if (WARN_ON(upper_32_bits(iova)))
+	if (WARN_ON(iova >= (1ULL << data->iop.cfg.ias)))
 		return 0;
 
 	return __arm_v7s_unmap(data, gather, iova, size, 1, data->pgd);
@@ -734,7 +745,7 @@ static phys_addr_t arm_v7s_iova_to_phys(struct io_pgtable_ops *ops,
 	u32 mask;
 
 	do {
-		ptep += ARM_V7S_LVL_IDX(iova, ++lvl);
+		ptep += ARM_V7S_LVL_IDX(iova, ++lvl, &data->iop.cfg);
 		pte = READ_ONCE(*ptep);
 		ptep = iopte_deref(pte, lvl, data);
 	} while (ARM_V7S_PTE_IS_TABLE(pte, lvl));
@@ -753,10 +764,10 @@ static struct io_pgtable *arm_v7s_alloc_pgtable(struct io_pgtable_cfg *cfg,
 {
 	struct arm_v7s_io_pgtable *data;
 
-	if (cfg->ias > ARM_V7S_ADDR_BITS)
+	if (cfg->ias > (arm_v7s_is_mtk_enabled(cfg) ? 34 : ARM_V7S_ADDR_BITS))
 		return NULL;
 
-	if (cfg->oas > (arm_v7s_is_mtk_enabled(cfg) ? 34 : ARM_V7S_ADDR_BITS))
+	if (cfg->oas > (arm_v7s_is_mtk_enabled(cfg) ? 35 : ARM_V7S_ADDR_BITS))
 		return NULL;
 
 	if (cfg->quirks & ~(IO_PGTABLE_QUIRK_ARM_NS |
@@ -777,8 +788,8 @@ static struct io_pgtable *arm_v7s_alloc_pgtable(struct io_pgtable_cfg *cfg,
 
 	spin_lock_init(&data->split_lock);
 	data->l2_tables = kmem_cache_create("io-pgtable_armv7s_l2",
-					    ARM_V7S_TABLE_SIZE(2),
-					    ARM_V7S_TABLE_SIZE(2),
+					    ARM_V7S_TABLE_SIZE(2, cfg),
+					    ARM_V7S_TABLE_SIZE(2, cfg),
 					    ARM_V7S_TABLE_SLAB_FLAGS, NULL);
 	if (!data->l2_tables)
 		goto out_free_data;
diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c
index ca51036aa..6ebfcc0f9 100644
--- a/drivers/iommu/io-pgtable-arm.c
+++ b/drivers/iommu/io-pgtable-arm.c
@@ -228,21 +228,18 @@ static dma_addr_t __arm_lpae_dma_addr(void *pages)
 }
 
 static void *__arm_lpae_alloc_pages(size_t size, gfp_t gfp,
-				    struct io_pgtable_cfg *cfg)
+				    struct io_pgtable_cfg *cfg, void *cookie)
 {
 	struct device *dev = cfg->iommu_dev;
 	int order = get_order(size);
-	struct page *p;
 	dma_addr_t dma;
 	void *pages;
 
 	VM_BUG_ON((gfp & __GFP_HIGHMEM));
-	p = alloc_pages_node(dev ? dev_to_node(dev) : NUMA_NO_NODE,
-			     gfp | __GFP_ZERO, order);
-	if (!p)
+	pages = io_pgtable_alloc_pages(cfg, cookie, order, gfp | __GFP_ZERO);
+	if (!pages)
 		return NULL;
 
-	pages = page_address(p);
 	if (!cfg->coherent_walk) {
 		dma = dma_map_single(dev, pages, size, DMA_TO_DEVICE);
 		if (dma_mapping_error(dev, dma))
@@ -262,17 +259,17 @@ static void *__arm_lpae_alloc_pages(size_t size, gfp_t gfp,
 	dev_err(dev, "Cannot accommodate DMA translation for IOMMU page tables\n");
 	dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);
 out_free:
-	__free_pages(p, order);
+	io_pgtable_free_pages(cfg, cookie, pages, order);
 	return NULL;
 }
 
 static void __arm_lpae_free_pages(void *pages, size_t size,
-				  struct io_pgtable_cfg *cfg)
+				  struct io_pgtable_cfg *cfg, void *cookie)
 {
 	if (!cfg->coherent_walk)
 		dma_unmap_single(cfg->iommu_dev, __arm_lpae_dma_addr(pages),
 				 size, DMA_TO_DEVICE);
-	free_pages((unsigned long)pages, get_order(size));
+	io_pgtable_free_pages(cfg, cookie, pages, get_order(size));
 }
 
 static void __arm_lpae_sync_pte(arm_lpae_iopte *ptep,
@@ -387,6 +384,7 @@ static int __arm_lpae_map(struct arm_lpae_io_pgtable *data, unsigned long iova,
 	size_t block_size = ARM_LPAE_BLOCK_SIZE(lvl, data);
 	size_t tblsz = ARM_LPAE_GRANULE(data);
 	struct io_pgtable_cfg *cfg = &data->iop.cfg;
+	void *cookie = data->iop.cookie;
 
 	/* Find our entry at the current level */
 	ptep += ARM_LPAE_LVL_IDX(iova, lvl, data);
@@ -402,13 +400,13 @@ static int __arm_lpae_map(struct arm_lpae_io_pgtable *data, unsigned long iova,
 	/* Grab a pointer to the next level */
 	pte = READ_ONCE(*ptep);
 	if (!pte) {
-		cptep = __arm_lpae_alloc_pages(tblsz, GFP_ATOMIC, cfg);
+		cptep = __arm_lpae_alloc_pages(tblsz, GFP_ATOMIC, cfg, cookie);
 		if (!cptep)
 			return -ENOMEM;
 
 		pte = arm_lpae_install_table(cptep, ptep, 0, cfg);
 		if (pte)
-			__arm_lpae_free_pages(cptep, tblsz, cfg);
+			__arm_lpae_free_pages(cptep, tblsz, cfg, cookie);
 	} else if (!cfg->coherent_walk && !(pte & ARM_LPAE_PTE_SW_SYNC)) {
 		__arm_lpae_sync_pte(ptep, cfg);
 	}
@@ -507,6 +505,7 @@ static void __arm_lpae_free_pgtable(struct arm_lpae_io_pgtable *data, int lvl,
 {
 	arm_lpae_iopte *start, *end;
 	unsigned long table_size;
+	void *cookie = data->iop.cookie;
 
 	if (lvl == ARM_LPAE_START_LVL(data))
 		table_size = data->pgd_size;
@@ -530,7 +529,7 @@ static void __arm_lpae_free_pgtable(struct arm_lpae_io_pgtable *data, int lvl,
 		__arm_lpae_free_pgtable(data, lvl + 1, iopte_deref(pte, data));
 	}
 
-	__arm_lpae_free_pages(start, table_size, &data->iop.cfg);
+	__arm_lpae_free_pages(start, table_size, &data->iop.cfg, cookie);
 }
 
 static void arm_lpae_free_pgtable(struct io_pgtable *iop)
@@ -553,11 +552,12 @@ static size_t arm_lpae_split_blk_unmap(struct arm_lpae_io_pgtable *data,
 	size_t tablesz = ARM_LPAE_GRANULE(data);
 	size_t split_sz = ARM_LPAE_BLOCK_SIZE(lvl, data);
 	int i, unmap_idx = -1;
+	void *cookie = data->iop.cookie;
 
 	if (WARN_ON(lvl == ARM_LPAE_MAX_LEVELS))
 		return 0;
 
-	tablep = __arm_lpae_alloc_pages(tablesz, GFP_ATOMIC, cfg);
+	tablep = __arm_lpae_alloc_pages(tablesz, GFP_ATOMIC, cfg, cookie);
 	if (!tablep)
 		return 0; /* Bytes unmapped */
 
@@ -577,7 +577,7 @@ static size_t arm_lpae_split_blk_unmap(struct arm_lpae_io_pgtable *data,
 
 	pte = arm_lpae_install_table(tablep, ptep, blk_pte, cfg);
 	if (pte != blk_pte) {
-		__arm_lpae_free_pages(tablep, tablesz, cfg);
+		__arm_lpae_free_pages(tablep, tablesz, cfg, cookie);
 		/*
 		 * We may race against someone unmapping another part of this
 		 * block, but anything else is invalid. We can't misinterpret
@@ -868,7 +868,8 @@ arm_64_lpae_alloc_pgtable_s1(struct io_pgtable_cfg *cfg, void *cookie)
 	cfg->arm_lpae_s1_cfg.mair[1] = 0;
 
 	/* Looking good; allocate a pgd */
-	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg);
+	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg,
+					   cookie);
 	if (!data->pgd)
 		goto out_free_data;
 
@@ -965,7 +966,8 @@ arm_64_lpae_alloc_pgtable_s2(struct io_pgtable_cfg *cfg, void *cookie)
 	cfg->arm_lpae_s2_cfg.vtcr = reg;
 
 	/* Allocate pgd pages */
-	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg);
+	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg,
+					   cookie);
 	if (!data->pgd)
 		goto out_free_data;
 
@@ -1053,7 +1055,8 @@ arm_mali_lpae_alloc_pgtable(struct io_pgtable_cfg *cfg, void *cookie)
 		(ARM_MALI_LPAE_MEMATTR_IMP_DEF
 		 << ARM_LPAE_MAIR_ATTR_SHIFT(ARM_LPAE_MAIR_ATTR_IDX_DEV));
 
-	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg);
+	data->pgd = __arm_lpae_alloc_pages(data->pgd_size, GFP_KERNEL, cfg,
+					   cookie);
 	if (!data->pgd)
 		goto out_free_data;
 
diff --git a/drivers/iommu/io-pgtable.c b/drivers/iommu/io-pgtable.c
index ced53e5b7..f0733cdbe 100644
--- a/drivers/iommu/io-pgtable.c
+++ b/drivers/iommu/io-pgtable.c
@@ -68,3 +68,36 @@ void free_io_pgtable_ops(struct io_pgtable_ops *ops)
 	io_pgtable_init_table[iop->fmt]->free(iop);
 }
 EXPORT_SYMBOL_GPL(free_io_pgtable_ops);
+
+void *io_pgtable_alloc_pages(struct io_pgtable_cfg *cfg, void *cookie,
+			     int order, gfp_t gfp_mask)
+{
+	struct device *dev;
+	struct page *p;
+
+	if (!cfg)
+		return NULL;
+
+	if (cfg->iommu_pgtable_ops && cfg->iommu_pgtable_ops->alloc_pgtable)
+		return cfg->iommu_pgtable_ops->alloc_pgtable(cookie, order,
+							     gfp_mask);
+
+	dev = cfg->iommu_dev;
+	p =  alloc_pages_node(dev ? dev_to_node(dev) : NUMA_NO_NODE,
+			      gfp_mask, order);
+	if (!p)
+		return NULL;
+	return page_address(p);
+}
+
+void io_pgtable_free_pages(struct io_pgtable_cfg *cfg, void *cookie, void *virt,
+			   int order)
+{
+	if (!cfg)
+		return;
+
+	if (cfg->iommu_pgtable_ops && cfg->iommu_pgtable_ops->free_pgtable)
+		cfg->iommu_pgtable_ops->free_pgtable(cookie, virt, order);
+	else
+		free_pages((unsigned long)virt, order);
+}
diff --git a/drivers/iommu/iommu-sysfs.c b/drivers/iommu/iommu-sysfs.c
index e436ff813..99869217f 100644
--- a/drivers/iommu/iommu-sysfs.c
+++ b/drivers/iommu/iommu-sysfs.c
@@ -87,6 +87,7 @@ int iommu_device_sysfs_add(struct iommu_device *iommu,
 	put_device(iommu->dev);
 	return ret;
 }
+EXPORT_SYMBOL_GPL(iommu_device_sysfs_add);
 
 void iommu_device_sysfs_remove(struct iommu_device *iommu)
 {
@@ -94,6 +95,8 @@ void iommu_device_sysfs_remove(struct iommu_device *iommu)
 	device_unregister(iommu->dev);
 	iommu->dev = NULL;
 }
+EXPORT_SYMBOL_GPL(iommu_device_sysfs_remove);
+
 /*
  * IOMMU drivers can indicate a device is managed by a given IOMMU using
  * this interface.  A link to the device will be created in the "devices"
@@ -119,6 +122,7 @@ int iommu_device_link(struct iommu_device *iommu, struct device *link)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(iommu_device_link);
 
 void iommu_device_unlink(struct iommu_device *iommu, struct device *link)
 {
@@ -128,3 +132,4 @@ void iommu_device_unlink(struct iommu_device *iommu, struct device *link)
 	sysfs_remove_link(&link->kobj, "iommu");
 	sysfs_remove_link_from_group(&iommu->dev->kobj, "devices", dev_name(link));
 }
+EXPORT_SYMBOL_GPL(iommu_device_unlink);
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 9d7232e26..c3d8e935a 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -22,6 +22,7 @@
 #include <linux/bitops.h>
 #include <linux/property.h>
 #include <linux/fsl/mc.h>
+#include <linux/module.h>
 #include <trace/events/iommu.h>
 
 static struct kset *iommu_group_kset;
@@ -141,6 +142,7 @@ int iommu_device_register(struct iommu_device *iommu)
 	spin_unlock(&iommu_device_lock);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(iommu_device_register);
 
 void iommu_device_unregister(struct iommu_device *iommu)
 {
@@ -148,6 +150,7 @@ void iommu_device_unregister(struct iommu_device *iommu)
 	list_del(&iommu->list);
 	spin_unlock(&iommu_device_lock);
 }
+EXPORT_SYMBOL_GPL(iommu_device_unregister);
 
 static struct iommu_param *iommu_get_dev_param(struct device *dev)
 {
@@ -183,10 +186,21 @@ int iommu_probe_device(struct device *dev)
 	if (!iommu_get_dev_param(dev))
 		return -ENOMEM;
 
+	if (!try_module_get(ops->owner)) {
+		ret = -EINVAL;
+		goto err_free_dev_param;
+	}
+
 	ret = ops->add_device(dev);
 	if (ret)
-		iommu_free_dev_param(dev);
+		goto err_module_put;
 
+	return 0;
+
+err_module_put:
+	module_put(ops->owner);
+err_free_dev_param:
+	iommu_free_dev_param(dev);
 	return ret;
 }
 
@@ -197,7 +211,10 @@ void iommu_release_device(struct device *dev)
 	if (dev->iommu_group)
 		ops->remove_device(dev);
 
-	iommu_free_dev_param(dev);
+	if (dev->iommu_param) {
+		module_put(ops->owner);
+		iommu_free_dev_param(dev);
+	}
 }
 
 static struct iommu_domain *__iommu_domain_alloc(struct bus_type *bus,
@@ -887,6 +904,7 @@ struct iommu_group *iommu_group_ref_get(struct iommu_group *group)
 	kobject_get(group->devices_kobj);
 	return group;
 }
+EXPORT_SYMBOL_GPL(iommu_group_ref_get);
 
 /**
  * iommu_group_put - Decrement group reference
@@ -1260,6 +1278,7 @@ struct iommu_group *generic_device_group(struct device *dev)
 {
 	return iommu_group_alloc();
 }
+EXPORT_SYMBOL_GPL(generic_device_group);
 
 /*
  * Use standard PCI bus topology, isolation features, and DMA alias quirks
@@ -1327,6 +1346,7 @@ struct iommu_group *pci_device_group(struct device *dev)
 	/* No shared group found, allocate new */
 	return iommu_group_alloc();
 }
+EXPORT_SYMBOL_GPL(pci_device_group);
 
 /* Get the IOMMU group for device on fsl-mc bus */
 struct iommu_group *fsl_mc_device_group(struct device *dev)
@@ -1339,6 +1359,7 @@ struct iommu_group *fsl_mc_device_group(struct device *dev)
 		group = iommu_group_alloc();
 	return group;
 }
+EXPORT_SYMBOL_GPL(fsl_mc_device_group);
 
 /**
  * iommu_group_get_for_dev - Find or create the IOMMU group for a device
@@ -1407,6 +1428,7 @@ struct iommu_group *iommu_group_get_for_dev(struct device *dev)
 
 	return group;
 }
+EXPORT_SYMBOL_GPL(iommu_group_get_for_dev);
 
 struct iommu_domain *iommu_group_default_domain(struct iommu_group *group)
 {
@@ -1537,6 +1559,11 @@ int bus_set_iommu(struct bus_type *bus, const struct iommu_ops *ops)
 {
 	int err;
 
+	if (ops == NULL) {
+		bus->iommu_ops = NULL;
+		return 0;
+	}
+
 	if (bus->iommu_ops != NULL)
 		return -EBUSY;
 
@@ -1653,8 +1680,10 @@ int iommu_attach_device(struct iommu_domain *domain, struct device *dev)
 	 */
 	mutex_lock(&group->mutex);
 	ret = -EINVAL;
-	if (iommu_group_device_count(group) != 1)
+	if (iommu_group_device_count(group) != 1) {
+		ret =  __iommu_attach_device(domain, dev);
 		goto out_unlock;
+	}
 
 	ret = __iommu_attach_group(domain, group);
 
@@ -1999,7 +2028,9 @@ size_t iommu_map_sg(struct iommu_domain *domain, unsigned long iova,
 	phys_addr_t start;
 	unsigned int i = 0;
 	int ret;
+	struct iommu_iotlb_gather iotlb_gather;
 
+	prot |= (1 << 16);
 	while (i <= nents) {
 		phys_addr_t s_phys = sg_phys(sg);
 
@@ -2023,6 +2054,10 @@ size_t iommu_map_sg(struct iommu_domain *domain, unsigned long iova,
 			sg = sg_next(sg);
 	}
 
+	iotlb_gather.start = iova;
+	iotlb_gather.end = iova + mapped;
+	if (domain->ops->iotlb_sync)
+		domain->ops->iotlb_sync(domain, &iotlb_gather);
 	return mapped;
 
 out_err:
@@ -2186,6 +2221,7 @@ struct iommu_resv_region *iommu_alloc_resv_region(phys_addr_t start,
 	region->type = type;
 	return region;
 }
+EXPORT_SYMBOL_GPL(iommu_alloc_resv_region);
 
 static int
 request_default_domain_for_dev(struct device *dev, unsigned long type)
diff --git a/drivers/iommu/iova.c b/drivers/iommu/iova.c
index 0e6a9536e..4eec92be8 100644
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@ -50,6 +50,7 @@ init_iova_domain(struct iova_domain *iovad, unsigned long granule,
 	iovad->anchor.pfn_lo = iovad->anchor.pfn_hi = IOVA_ANCHOR;
 	rb_link_node(&iovad->anchor.node, NULL, &iovad->rbroot.rb_node);
 	rb_insert_color(&iovad->anchor.node, &iovad->rbroot);
+	iovad->best_fit = false;
 	init_iova_rcaches(iovad);
 }
 EXPORT_SYMBOL_GPL(init_iova_domain);
@@ -177,6 +178,24 @@ iova_insert_rbtree(struct rb_root *root, struct iova *iova,
 	rb_insert_color(&iova->node, root);
 }
 
+#ifdef CONFIG_IOMMU_LIMIT_IOVA_ALIGNMENT
+static unsigned long limit_align_shift(struct iova_domain *iovad,
+				       unsigned long shift)
+{
+	unsigned long max_align_shift;
+
+	max_align_shift = CONFIG_IOMMU_IOVA_ALIGNMENT + PAGE_SHIFT
+		- iova_shift(iovad);
+	return min_t(unsigned long, max_align_shift, shift);
+}
+#else
+static unsigned long limit_align_shift(struct iova_domain *iovad,
+				       unsigned long shift)
+{
+	return shift;
+}
+#endif
+
 static int __alloc_and_insert_iova_range(struct iova_domain *iovad,
 		unsigned long size, unsigned long limit_pfn,
 			struct iova *new, bool size_aligned)
@@ -188,7 +207,7 @@ static int __alloc_and_insert_iova_range(struct iova_domain *iovad,
 	unsigned long align_mask = ~0UL;
 
 	if (size_aligned)
-		align_mask <<= fls_long(size - 1);
+		align_mask <<= limit_align_shift(iovad, fls_long(size - 1));
 
 	/* Walk the tree backwards */
 	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
@@ -227,6 +246,70 @@ static int __alloc_and_insert_iova_range(struct iova_domain *iovad,
 	return -ENOMEM;
 }
 
+static int __alloc_and_insert_iova_best_fit(struct iova_domain *iovad,
+					    unsigned long size,
+					    unsigned long limit_pfn,
+					    struct iova *new, bool size_aligned)
+{
+	struct rb_node *curr, *prev;
+	struct iova *curr_iova, *prev_iova;
+	unsigned long flags;
+	unsigned long align_mask = ~0UL;
+	struct rb_node *candidate_rb_parent;
+	unsigned long new_pfn, candidate_pfn = ~0UL;
+	unsigned long gap, candidate_gap = ~0UL;
+
+	if (size_aligned)
+		align_mask <<= limit_align_shift(iovad, fls_long(size - 1));
+
+	/* Walk the tree backwards */
+	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
+	curr = &iovad->anchor.node;
+	prev = rb_prev(curr);
+	for (; prev; curr = prev, prev = rb_prev(curr)) {
+		curr_iova = rb_entry(curr, struct iova, node);
+		prev_iova = rb_entry(prev, struct iova, node);
+
+		limit_pfn = min(limit_pfn, curr_iova->pfn_lo);
+		new_pfn = (limit_pfn - size) & align_mask;
+		gap = curr_iova->pfn_lo - prev_iova->pfn_hi - 1;
+		if ((limit_pfn >= size) && (new_pfn > prev_iova->pfn_hi)
+				&& (gap < candidate_gap)) {
+			candidate_gap = gap;
+			candidate_pfn = new_pfn;
+			candidate_rb_parent = curr;
+			if (gap == size)
+				goto insert;
+		}
+	}
+
+	curr_iova = rb_entry(curr, struct iova, node);
+	limit_pfn = min(limit_pfn, curr_iova->pfn_lo);
+	new_pfn = (limit_pfn - size) & align_mask;
+	gap = curr_iova->pfn_lo - iovad->start_pfn;
+	if (limit_pfn >= size && new_pfn >= iovad->start_pfn &&
+			gap < candidate_gap) {
+		candidate_gap = gap;
+		candidate_pfn = new_pfn;
+		candidate_rb_parent = curr;
+	}
+
+insert:
+	if (candidate_pfn == ~0UL) {
+		spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+		return -ENOMEM;
+	}
+
+	/* pfn_lo will point to size aligned address if size_aligned is set */
+	new->pfn_lo = candidate_pfn;
+	new->pfn_hi = new->pfn_lo + size - 1;
+
+	/* If we have 'prev', it's a valid place to start the insertion. */
+	iova_insert_rbtree(&iovad->rbroot, new, candidate_rb_parent);
+	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+	return 0;
+}
+
 static struct kmem_cache *iova_cache;
 static unsigned int iova_cache_users;
 static DEFINE_MUTEX(iova_cache_mutex);
@@ -302,8 +385,13 @@ alloc_iova(struct iova_domain *iovad, unsigned long size,
 	if (!new_iova)
 		return NULL;
 
-	ret = __alloc_and_insert_iova_range(iovad, size, limit_pfn + 1,
-			new_iova, size_aligned);
+	if (iovad->best_fit) {
+		ret = __alloc_and_insert_iova_best_fit(iovad, size,
+				limit_pfn + 1, new_iova, size_aligned);
+	} else {
+		ret = __alloc_and_insert_iova_range(iovad, size, limit_pfn + 1,
+				new_iova, size_aligned);
+	}
 
 	if (ret) {
 		free_iova_mem(new_iova);
diff --git a/drivers/iommu/of_iommu.c b/drivers/iommu/of_iommu.c
index 614a93aa5..25491403a 100644
--- a/drivers/iommu/of_iommu.c
+++ b/drivers/iommu/of_iommu.c
@@ -8,6 +8,7 @@
 #include <linux/export.h>
 #include <linux/iommu.h>
 #include <linux/limits.h>
+#include <linux/module.h>
 #include <linux/of.h>
 #include <linux/of_iommu.h>
 #include <linux/of_pci.h>
@@ -89,16 +90,16 @@ static int of_iommu_xlate(struct device *dev,
 {
 	const struct iommu_ops *ops;
 	struct fwnode_handle *fwnode = &iommu_spec->np->fwnode;
-	int err;
+	int ret;
 
 	ops = iommu_ops_from_fwnode(fwnode);
 	if ((ops && !ops->of_xlate) ||
 	    !of_device_is_available(iommu_spec->np))
 		return NO_IOMMU;
 
-	err = iommu_fwspec_init(dev, &iommu_spec->np->fwnode, ops);
-	if (err)
-		return err;
+	ret = iommu_fwspec_init(dev, &iommu_spec->np->fwnode, ops);
+	if (ret)
+		return ret;
 	/*
 	 * The otherwise-empty fwspec handily serves to indicate the specific
 	 * IOMMU device we're waiting for, which will be useful if we ever get
@@ -107,7 +108,12 @@ static int of_iommu_xlate(struct device *dev,
 	if (!ops)
 		return driver_deferred_probe_check_state(dev);
 
-	return ops->of_xlate(dev, iommu_spec);
+	if (!try_module_get(ops->owner))
+		return -ENODEV;
+
+	ret = ops->of_xlate(dev, iommu_spec);
+	module_put(ops->owner);
+	return ret;
 }
 
 struct of_pci_iommu_alias_info {
@@ -177,6 +183,7 @@ const struct iommu_ops *of_iommu_configure(struct device *dev,
 			.np = master_np,
 		};
 
+		pci_request_acs();
 		err = pci_for_each_dma_alias(to_pci_dev(dev),
 					     of_pci_iommu_init, &info);
 	} else if (dev_is_fsl_mc(dev)) {
diff --git a/drivers/iommu/sunxi-iommu-debug.c b/drivers/iommu/sunxi-iommu-debug.c
new file mode 100644
index 000000000..055f2de00
--- /dev/null
+++ b/drivers/iommu/sunxi-iommu-debug.c
@@ -0,0 +1,952 @@
+/*
+ * Copyright (c) 2015-2016, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/iommu.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/dma-contiguous.h>
+#include <linux/dma-mapping.h>
+#include <asm/cacheflush.h>
+#include <asm/barrier.h>
+#include "sunxi-iommu.h"
+#include <linux/iommu.h>
+#include <asm/cacheflush.h>
+#include <linux/module.h>
+#include <linux/compiler.h>
+
+
+#ifdef CONFIG_SUNXI_IOMMU_TESTS
+
+#ifdef CONFIG_64BIT
+
+#define kstrtoux kstrtou64
+#define kstrtox_from_user kstrtoll_from_user
+#define kstrtosize_t kstrtoul
+
+#else
+
+#define kstrtoux kstrtou32
+#define kstrtox_from_user kstrtoint_from_user
+#define kstrtosize_t kstrtouint
+
+#endif
+
+#define ION_KERNEL_USER_ERR(str)	pr_err("%s failed!", #str)
+struct ion_facade {
+	struct ion_client *client;
+	struct ion_handle *handle;
+	dma_addr_t dma_address;
+	void *virtual_address;
+	size_t address_length;
+	struct sg_table *sg_table;
+};
+
+static struct dentry *iommu_debugfs_top;
+
+static LIST_HEAD(iommu_debug_devices);
+static struct dentry *debugfs_tests_dir;
+
+struct iommu_debug_device {
+	struct device *dev;
+	struct iommu_domain *domain;
+	u64 iova;
+	u64 phys;
+	size_t len;
+	struct list_head list;
+};
+
+static const char * const _size_to_string(unsigned long size)
+{
+	switch (size) {
+	case SZ_4K:
+		return "4K";
+	case SZ_8K:
+		return "8K";
+	case SZ_16K:
+		return "16K";
+	case SZ_64K:
+		return "64K";
+	case SZ_2M:
+		return "2M";
+	case SZ_1M:
+		return "1M";
+	case SZ_1M * 4:
+		return "4M";
+	case SZ_1M * 8:
+		return "8M";
+	case SZ_1M * 16:
+		return "16M";
+	case SZ_1M * 32:
+		return "32M";
+	}
+	return "unknown size, please add to _size_to_string";
+}
+
+static int iommu_debug_profiling_fast_dma_api_show(struct seq_file *s,
+						 void *ignored)
+{
+	int i, experiment;
+	struct iommu_debug_device *ddev = s->private;
+	struct device *dev = ddev->dev;
+	u64 map_elapsed_ns[10], unmap_elapsed_ns[10];
+	dma_addr_t dma_addr;
+	void *virt;
+	const char * const extra_labels[] = {
+		"not coherent",
+		"coherent",
+	};
+	unsigned long extra_attrs[] = {
+		0,
+		DMA_ATTR_SKIP_CPU_SYNC,
+	};
+
+	virt = kmalloc(1518, GFP_KERNEL);
+	if (!virt)
+		goto out;
+
+	for (experiment = 0; experiment < 2; ++experiment) {
+		size_t map_avg = 0, unmap_avg = 0;
+
+		for (i = 0; i < 10; ++i) {
+			struct timespec tbefore, tafter, diff;
+			u64 ns;
+
+			getnstimeofday(&tbefore);
+			dma_addr = dma_map_single_attrs(
+				dev, virt, SZ_4K, DMA_TO_DEVICE,
+				extra_attrs[experiment]);
+			getnstimeofday(&tafter);
+			diff.tv_sec = tafter.tv_sec - tbefore.tv_sec;
+			diff.tv_nsec = tafter.tv_nsec - tbefore.tv_nsec;
+			ns = timespec_to_ns(&diff);
+			if (dma_mapping_error(dev, dma_addr)) {
+				seq_puts(s, "dma_map_single failed\n");
+				goto out_disable_config_clocks;
+			}
+			map_elapsed_ns[i] = ns;
+			getnstimeofday(&tbefore);
+			dma_unmap_single_attrs(
+				dev, dma_addr, SZ_4K, DMA_TO_DEVICE,
+				extra_attrs[experiment]);
+			getnstimeofday(&tafter);
+			diff.tv_sec = tafter.tv_sec - tbefore.tv_sec;
+			diff.tv_nsec = tafter.tv_nsec - tbefore.tv_nsec;
+			ns = timespec_to_ns(&diff);
+			unmap_elapsed_ns[i] = ns;
+		}
+		seq_printf(s, "%13s %24s (ns): [", extra_labels[experiment],
+			   "dma_map_single_attrs");
+		for (i = 0; i < 10; ++i) {
+			map_avg += map_elapsed_ns[i];
+			seq_printf(s, "%5llu%s", map_elapsed_ns[i],
+				   i < 9 ? ", " : "");
+		}
+		map_avg /= 10;
+		seq_printf(s, "] (avg: %zu)\n", map_avg);
+
+		seq_printf(s, "%13s %24s (ns): [", extra_labels[experiment],
+			   "dma_unmap_single_attrs");
+		for (i = 0; i < 10; ++i) {
+			unmap_avg += unmap_elapsed_ns[i];
+			seq_printf(s, "%5llu%s", unmap_elapsed_ns[i],
+				   i < 9 ? ", " : "");
+		}
+		unmap_avg /= 10;
+		seq_printf(s, "] (avg: %zu)\n", unmap_avg);
+	}
+
+out_disable_config_clocks:
+	kfree(virt);
+out:
+	return 0;
+}
+
+static int iommu_debug_profiling_fast_dma_api_open(struct inode *inode,
+						 struct file *file)
+{
+	return single_open(file, iommu_debug_profiling_fast_dma_api_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_debug_profiling_fast_dma_api_fops = {
+	.open	 = iommu_debug_profiling_fast_dma_api_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+
+/* Creates a fresh fast mapping and applies @fn to it */
+static int __apply_to_new_mapping(struct seq_file *s,
+				    int (*fn)(struct device *dev,
+					      struct seq_file *s,
+					      struct iommu_domain *domain,
+					      void *priv),
+				    void *priv)
+{
+	struct iommu_debug_device *ddev = s->private;
+	struct device *dev = ddev->dev;
+	int ret = -EINVAL;
+
+	ret = fn(dev, s, global_iommu_domain, priv);
+	return ret;
+}
+
+#if 0
+static int __tlb_stress_sweep(struct device *dev, struct seq_file *s)
+{
+	int i, ret = 0;
+	unsigned long iova;
+	const unsigned long max = SZ_1G * 4UL;
+	void *virt;
+	phys_addr_t phys;
+	dma_addr_t dma_addr;
+
+	/*
+	 * we'll be doing 4K and 8K mappings.  Need to own an entire 8K
+	 * chunk that we can work with.
+	 */
+	virt = (void *)__get_free_pages(GFP_KERNEL, get_order(SZ_8K));
+	phys = virt_to_phys(virt);
+
+	/* fill the whole 4GB space */
+	for (iova = 0, i = 0; iova < max; iova += SZ_8K, ++i) {
+		dma_addr = dma_map_single(dev, virt, SZ_8K, DMA_TO_DEVICE);
+		if (dma_addr == 0) {
+			dev_err(dev, "Failed map on iter %d\n", i);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/*
+	 * free up 4K at the very beginning, then leave one 4K mapping,
+	 * then free up 8K.  This will result in the next 8K map to skip
+	 * over the 4K hole and take the 8K one.
+	 */
+	dma_unmap_single(dev, 0, SZ_4K, DMA_TO_DEVICE);
+	dma_unmap_single(dev, SZ_8K, SZ_4K, DMA_TO_DEVICE);
+	dma_unmap_single(dev, SZ_8K + SZ_4K, SZ_4K, DMA_TO_DEVICE);
+
+	/* remap 8K */
+	dma_addr = dma_map_single(dev, virt, SZ_8K, DMA_TO_DEVICE);
+
+	/*
+	 * now remap 4K.  We should get the first 4K chunk that was skipped
+	 * over during the previous 8K map.  If we missed a TLB invalidate
+	 * at that point this should explode.
+	 */
+	dma_addr = dma_map_single(dev, virt, SZ_4K, DMA_TO_DEVICE);
+
+
+	/* we're all full again. unmap everything. */
+	for (dma_addr = 0; dma_addr < max; dma_addr += SZ_8K)
+		dma_unmap_single(dev, dma_addr, SZ_8K, DMA_TO_DEVICE);
+
+out:
+	free_pages((unsigned long)virt, get_order(SZ_8K));
+	return ret;
+}
+#else
+static int __tlb_stress_sweep(struct device *dev, struct seq_file *s)
+{
+	return 0;
+}
+#endif
+
+struct fib_state {
+	unsigned long cur;
+	unsigned long prev;
+};
+
+static __maybe_unused void fib_init(struct fib_state *f)
+{
+	f->cur = f->prev = 1;
+}
+
+static __maybe_unused unsigned long get_next_fib(struct fib_state *f)
+{
+	int next = f->cur + f->prev;
+
+	f->prev = f->cur;
+	f->cur = next;
+	return next;
+}
+
+/*
+ * Not actually random.  Just testing the fibs (and max - the fibs).
+ */
+#if 0
+static int __rand_va_sweep(struct device *dev, struct seq_file *s,
+			   const size_t size)
+{
+	u64 iova;
+	const unsigned long max = SZ_1G * 4UL;
+	int i, remapped, unmapped, ret = 0;
+	void *virt;
+	dma_addr_t dma_addr, dma_addr2;
+	struct fib_state fib;
+
+	virt = (void *)__get_free_pages(GFP_KERNEL, get_order(size));
+	if (!virt) {
+		if (size > SZ_8K) {
+			dev_err(dev,
+				"Failed to allocate %s of memory, which is a lot. Skipping test for this size\n",
+				_size_to_string(size));
+			return 0;
+		}
+		return -ENOMEM;
+	}
+
+	/* fill the whole 4GB space */
+	for (iova = 0, i = 0; iova < max; iova += size, ++i) {
+		dma_addr = dma_map_single(dev, virt, size, DMA_TO_DEVICE);
+		if (dma_addr == 0) {
+			dev_err(dev, "Failed map on iter %d\n", i);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* now unmap "random" iovas */
+	unmapped = 0;
+	fib_init(&fib);
+	for (iova = get_next_fib(&fib) * size;
+	     iova < max - size;
+	     iova = get_next_fib(&fib) * size) {
+		dma_addr = iova;
+		dma_addr2 = max - size - iova;
+		if (dma_addr == dma_addr2) {
+			WARN(1,
+			"%s test needs update! The random number sequence is folding in on itself and should be changed.\n",
+			__func__);
+			return -EINVAL;
+		}
+		dma_unmap_single(dev, dma_addr, size, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dma_addr2, size, DMA_TO_DEVICE);
+		unmapped += 2;
+	}
+
+	/* and map until everything fills back up */
+	for (remapped = 0;; ++remapped) {
+		dma_addr = dma_map_single(dev, virt, size, DMA_TO_DEVICE);
+		if (dma_addr == 0)
+			break;
+	}
+
+	if (unmapped != remapped) {
+		dev_err(dev,
+			"Unexpected random remap count! Unmapped %d but remapped %d\n",
+			unmapped, remapped);
+		ret = -EINVAL;
+	}
+
+	for (dma_addr = 0; dma_addr < max; dma_addr += size)
+		dma_unmap_single(dev, dma_addr, size, DMA_TO_DEVICE);
+
+out:
+	free_pages((unsigned long)virt, get_order(size));
+	return ret;
+}
+#else
+static int __rand_va_sweep(struct device *dev, struct seq_file *s,
+			   const size_t size)
+{
+	return 0;
+}
+#endif
+
+struct dma_addr_list {
+	dma_addr_t addr;
+	void *next;
+};
+
+static int __full_va_sweep(struct device *dev, struct seq_file *s,
+			   const size_t size, struct iommu_domain *domain)
+{
+	unsigned long iova;
+	void *virt;
+	phys_addr_t phys;
+	const unsigned long max = SZ_128M;
+	struct dma_addr_list *phead, *p, *ptmp;
+
+	phead = kmalloc(sizeof(struct dma_addr_list), GFP_KERNEL);
+	memset(phead, 0, sizeof(struct dma_addr_list));
+	virt = (void *)__get_free_pages(GFP_KERNEL, get_order(size));
+	if (!virt) {
+		if (size > SZ_8K) {
+			dev_err(dev,
+				"Failed to allocate %s of memory, which is a lot. Skipping test for this size\n",
+				_size_to_string(size));
+			return 0;
+		}
+		return -ENOMEM;
+	}
+	phys = virt_to_phys(virt);
+
+	for (p = phead, iova = 0; iova < max; iova += size, p = p->next) {
+		p->addr = dma_map_single(dev, virt, size, DMA_TO_DEVICE);
+		p->next = kmalloc(sizeof(struct dma_addr_list), GFP_KERNEL);
+		memset(p->next, 0, sizeof(struct dma_addr_list));
+		if (p->addr == 0 || p->addr == DMA_MAPPING_ERROR) {
+			dev_err(dev, "Failed to map dma, out of iova space\n");
+			return -ENOMEM;
+		}
+	}
+
+	for (p = phead; p->addr != 0;) {
+		dma_unmap_single(dev, p->addr, size, DMA_TO_DEVICE);
+		ptmp = p;
+		p = p->next;
+		kfree(ptmp);
+	}
+	kfree(p);
+	free_pages((unsigned long)virt, get_order(size));
+	return 0;
+}
+
+#define ds_printf(d, s, fmt, ...) ({				\
+			dev_err(d, fmt, ##__VA_ARGS__);		\
+			seq_printf(s, fmt, ##__VA_ARGS__);	\
+		})
+
+static int __functional_dma_api_va_test(struct device *dev, struct seq_file *s,
+				     struct iommu_domain *domain, void *priv)
+{
+	int i, j;
+	int ret = 0;
+	size_t *sz, *sizes = priv;
+
+	for (j = 0; j < 1; ++j) {
+		for (sz = sizes; *sz; ++sz) {
+			for (i = 0; i < 2; ++i) {
+				ds_printf(dev, s, "Full VA sweep @%s %d",
+					       _size_to_string(*sz), i);
+				if (__full_va_sweep(dev, s, *sz, domain)) {
+					ds_printf(dev, s, "  -> FAILED\n");
+					ret = -EINVAL;
+					goto out;
+				} else
+					ds_printf(dev, s, "  -> SUCCEEDED\n");
+			}
+		}
+	}
+
+	ds_printf(dev, s, "bonus map:");
+	if (__full_va_sweep(dev, s, SZ_4K, domain)) {
+		ds_printf(dev, s, "  -> FAILED\n");
+		ret = -EINVAL;
+		goto out;
+	} else
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+
+	for (sz = sizes; *sz; ++sz) {
+		for (i = 0; i < 2; ++i) {
+			ds_printf(dev, s, "Rand VA sweep @%s %d",
+				   _size_to_string(*sz), i);
+			if (__rand_va_sweep(dev, s, *sz)) {
+				ds_printf(dev, s, "  -> FAILED\n");
+				ret = -EINVAL;
+				goto out;
+			} else
+				ds_printf(dev, s, "  -> SUCCEEDED\n");
+		}
+	}
+
+	ds_printf(dev, s, "TLB stress sweep");
+	if (__tlb_stress_sweep(dev, s)) {
+		ds_printf(dev, s, "  -> FAILED\n");
+		ret = -EINVAL;
+		goto out;
+	} else
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+
+	ds_printf(dev, s, "second bonus map:");
+	if (__full_va_sweep(dev, s, SZ_4K, domain)) {
+		ds_printf(dev, s, "  -> FAILED\n");
+		ret = -EINVAL;
+		goto out;
+	} else
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+out:
+	return ret;
+}
+
+/*iova alloc strategy stress test*/
+static int iommu_iova_alloc_strategy_stress_show(struct seq_file *s,
+						    void *ignored)
+{
+	size_t sizes[] = {SZ_4K, SZ_8K, SZ_16K, SZ_64K, 0};
+	int ret = 0;
+
+	ret = __apply_to_new_mapping(s, __functional_dma_api_va_test, sizes);
+	return ret;
+}
+
+static int iommu_iova_alloc_strategy_stress_open(struct inode *inode,
+						    struct file *file)
+{
+	return single_open(file, iommu_iova_alloc_strategy_stress_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_iova_alloc_strategy_stress_fops = {
+	.open	 = iommu_iova_alloc_strategy_stress_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+
+static int __functional_dma_api_alloc_test(struct device *dev,
+					   struct seq_file *s,
+					   struct iommu_domain *domain,
+					   void *ignored)
+{
+	size_t size = SZ_1K * 742;
+	int ret = 0;
+	u8 *data;
+	dma_addr_t iova;
+
+	/* Make sure we can allocate and use a buffer */
+	ds_printf(dev, s, "Allocating coherent buffer");
+	data = dma_alloc_coherent(dev, size, &iova, GFP_KERNEL);
+	if (!data) {
+		ds_printf(dev, s, "  -> FAILED\n");
+		ret = -EINVAL;
+	} else {
+		int i;
+
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+		ds_printf(dev, s, "Using coherent buffer");
+		for (i = 0; i < 742; ++i) {
+			int ind = SZ_1K * i;
+			u8 *p = data + ind;
+			u8 val = i % 255;
+
+			memset(data, 0xa5, size);
+			*p = val;
+			(*p)++;
+			if ((*p) != val + 1) {
+				ds_printf(dev, s,
+					  "  -> FAILED on iter %d since %d != %d\n",
+					  i, *p, val + 1);
+				ret = -EINVAL;
+				break;
+			}
+		}
+		if (!ret)
+			ds_printf(dev, s, "  -> SUCCEEDED\n");
+		dma_free_coherent(dev, size, data, iova);
+	}
+
+	return ret;
+}
+
+/*iommu kernel virtual addr read/write*/
+static int iommu_kvirtual_addr_rdwr_show(struct seq_file *s,
+						   void *ignored)
+{
+	struct iommu_debug_device *ddev = s->private;
+	struct device *dev = ddev->dev;
+	int ret = -EINVAL;
+
+	ret = __functional_dma_api_alloc_test(dev, s,
+				global_iommu_domain, NULL);
+	return ret;
+}
+
+static int iommu_kvirtual_addr_rdwr_open(struct inode *inode,
+						   struct file *file)
+{
+	return single_open(file, iommu_kvirtual_addr_rdwr_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_kvirtul_addr_rdwr_fops = {
+	.open	 = iommu_kvirtual_addr_rdwr_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+static int __functional_dma_api_ion_test(struct device *dev,
+					   struct seq_file *s,
+					   struct iommu_domain *domain,
+					   void *ignored)
+{
+	return 0;
+}
+
+/*iommu ion interface test*/
+static int iommu_ion_interface_test_show(struct seq_file *s,
+						   void *ignored)
+{
+	struct iommu_debug_device *ddev = s->private;
+	struct device *dev = ddev->dev;
+	int ret = -EINVAL;
+
+	ret = __functional_dma_api_ion_test(dev, s,
+				global_iommu_domain, NULL);
+	return ret;
+}
+
+static int iommu_ion_interface_test_open(struct inode *inode,
+						   struct file *file)
+{
+	return single_open(file, iommu_ion_interface_test_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_ion_interface_test_fops = {
+	.open	 = iommu_ion_interface_test_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+static int __functional_dma_api_iova_test(struct device *dev,
+					   struct seq_file *s,
+					   struct iommu_domain *domain,
+					   void *ignored)
+{
+	size_t size = SZ_4K * 1024;
+	int ret = 0;
+	u32 *data;
+	dma_addr_t iova;
+
+	sunxi_set_debug_mode();
+
+	/* Make sure we can allocate and use a buffer */
+	ds_printf(dev, s, "Allocating coherent iova buffer");
+	data = dma_alloc_coherent(dev, size, &iova, GFP_KERNEL);
+	if (!data) {
+		ds_printf(dev, s, "  -> FAILED\n");
+		ret = -EINVAL;
+	} else {
+		int i;
+
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+		ds_printf(dev, s, "Using coherent buffer");
+		for (i = 0; i < 1024; ++i) {
+			int ind = (SZ_4K * i) / sizeof(u32);
+			u32 *p = data + ind;
+			u32 *p1 = (u32 *)iova + ind;
+			u32 read_data;
+
+			memset(data, 0xa5, size);
+			*p = 0x5a5a5a5a;
+			/**
+			 * make sure that *p is written before
+			 * the write operation of the debug mode of iommu
+			 */
+			wmb();
+			sunxi_iova_test_write((dma_addr_t)p1, 0xdead);
+			/**
+			 * do the write operation of debug mode of iommu
+			 * in order
+			 */
+			rmb();
+			if ((*p) != 0xdead) {
+				ds_printf(dev, s,
+				 "-> FAILED on iova0 iter %x  %x\n", i, *p);
+				ret = -EINVAL;
+				goto out;
+			}
+
+			*p = 0xffffaaaa;
+			/**
+			 * make sure that *p is written before
+			 * the read operation of the debug mode of iommu
+			 */
+			wmb();
+			read_data = sunxi_iova_test_read((dma_addr_t)p1);
+			if (read_data != 0xffffaaaa) {
+				ds_printf(dev, s,
+					"-> FAILED on iova1 iter %x  %x\n",
+					i, read_data);
+				ret = -EINVAL;
+				goto out;
+			}
+
+		}
+		if (!ret)
+			ds_printf(dev, s, "  -> SUCCEEDED\n");
+	}
+out:
+	dma_free_coherent(dev, size, data, iova);
+	sunxi_set_prefetch_mode();
+	return ret;
+}
+
+/*iommu test use debug interface*/
+static int iommu_vir_devio_addr_rdwr_show(struct seq_file *s,
+						    void *ignored)
+{
+	int ret = 0;
+
+	ret = __apply_to_new_mapping(s, __functional_dma_api_iova_test, NULL);
+	if (ret) {
+		pr_err("the first iova test failed\n");
+		return ret;
+	}
+	ret = 0;
+	ret = __apply_to_new_mapping(s, __functional_dma_api_iova_test, NULL);
+	if (ret) {
+		pr_err("the second iova test failed\n");
+		return ret;
+	}
+	ret = 0;
+	ret = __apply_to_new_mapping(s, __functional_dma_api_iova_test, NULL);
+	if (ret) {
+		pr_err("the third iova test failed\n");
+		return ret;
+	}
+	return 0;
+}
+
+static int iommu_vir_devio_addr_rdwr_open(struct inode *inode,
+						    struct file *file)
+{
+	return single_open(file, iommu_vir_devio_addr_rdwr_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_vir_devio_addr_rdwr_fops = {
+	.open	 = iommu_vir_devio_addr_rdwr_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+
+static int __functional_dma_api_basic_test(struct device *dev,
+					   struct seq_file *s,
+					   struct iommu_domain *domain,
+					   void *ignored)
+{
+	size_t size = 1518;
+	int i, j, ret = 0;
+	u8 *data;
+	dma_addr_t iova;
+	phys_addr_t pa, pa2;
+
+	ds_printf(dev, s, "Basic DMA API test");
+	/* Make sure we can allocate and use a buffer */
+	for (i = 0; i < 1000; ++i) {
+		data = kmalloc(size, GFP_KERNEL);
+		if (!data) {
+			ds_printf(dev, s, "  -> FAILED\n");
+			ret = -EINVAL;
+			goto out;
+		}
+		memset(data, 0xa5, size);
+		iova = dma_map_single(dev, data, size, DMA_TO_DEVICE);
+		pa = iommu_iova_to_phys(domain, iova);
+		pa2 = virt_to_phys(data);
+		if (pa != pa2) {
+			dev_err(dev,
+				"iova_to_phys doesn't match virt_to_phys: %pa != %pa\n",
+				&pa, &pa2);
+			ret = -EINVAL;
+			kfree(data);
+			goto out;
+		}
+		dma_unmap_single(dev, iova, size, DMA_TO_DEVICE);
+		for (j = 0; j < size; ++j) {
+			if (data[j] != 0xa5) {
+				dev_err(dev, "data[%d] != 0xa5\n", data[j]);
+				ret = -EINVAL;
+				kfree(data);
+				goto out;
+			}
+		}
+		kfree(data);
+	}
+
+out:
+	if (ret)
+		ds_printf(dev, s, "  -> FAILED\n");
+	else
+		ds_printf(dev, s, "  -> SUCCEEDED\n");
+
+	return ret;
+}
+
+/*iommu basic test*/
+static int iommu_debug_basic_test_show(struct seq_file *s,
+						   void *ignored)
+{
+	struct iommu_debug_device *ddev = s->private;
+	struct device *dev = ddev->dev;
+	int ret = -EINVAL;
+
+	ret = __functional_dma_api_basic_test(dev, s,
+				global_iommu_domain, NULL);
+	return ret;
+}
+
+static int iommu_debug_basic_test_open(struct inode *inode,
+						   struct file *file)
+{
+	return single_open(file, iommu_debug_basic_test_show,
+			   inode->i_private);
+}
+
+static const struct file_operations iommu_debug_basic_test_fops = {
+	.open	 = iommu_debug_basic_test_open,
+	.read	 = seq_read,
+	.llseek	 = seq_lseek,
+	.release = single_release,
+};
+
+
+/*
+ * The following will only work for drivers that implement the generic
+ * device tree bindings described in
+ * Documentation/devicetree/bindings/iommu/iommu.txt
+ */
+static int snarf_iommu_devices(struct device *dev, const char *name)
+{
+	struct iommu_debug_device *ddev;
+	struct dentry *dir;
+
+	if (IS_ERR_OR_NULL(dev))
+		return -EINVAL;
+
+	ddev = kzalloc(sizeof(*ddev), GFP_KERNEL);
+	if (!ddev)
+		return -ENODEV;
+	ddev->dev = dev;
+	ddev->domain = global_iommu_domain;
+	dir = debugfs_create_dir(name, debugfs_tests_dir);
+	if (!dir) {
+		pr_err("Couldn't create iommu/devices/%s debugfs dir\n",
+		       name);
+		goto err;
+	}
+
+	if (!debugfs_create_file("profiling_fast_dma_api", 0400, dir, ddev,
+				 &iommu_debug_profiling_fast_dma_api_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/profiling_fast_dma_api debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	if (!debugfs_create_file("iommu_basic_test", 0400, dir, ddev,
+				 &iommu_debug_basic_test_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/iommu_basic_test debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	if (!debugfs_create_file("ion_interface_test", 0400, dir, ddev,
+				 &iommu_ion_interface_test_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/ion_interface_test debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	if (!debugfs_create_file("iova_alloc_strategy_stress_test",
+		0200, dir, ddev,
+		&iommu_iova_alloc_strategy_stress_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/iova_alloc_strategy_stress_test debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	if (!debugfs_create_file("kvirtual_addr_rdwr_test", 0200, dir, ddev,
+				 &iommu_kvirtul_addr_rdwr_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/kvirtual_addr_rdwr_test debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	if (!debugfs_create_file("vir_devio_addr_rdwr_test", 0200, dir, ddev,
+				 &iommu_vir_devio_addr_rdwr_fops)) {
+		pr_err("Couldn't create iommu/devices/%s/vir_devio_addr_rdwr_test debugfs file\n",
+		       name);
+		goto err_rmdir;
+	}
+
+	list_add(&ddev->list, &iommu_debug_devices);
+	return 0;
+
+err_rmdir:
+	debugfs_remove_recursive(dir);
+err:
+	kfree(ddev);
+	return 0;
+}
+
+static int pass_iommu_devices(struct device *dev, void *ignored)
+{
+	if (!of_find_property(dev->of_node, "iommus", NULL))
+		return 0;
+
+	return snarf_iommu_devices(dev, dev_name(dev));
+}
+
+static int iommu_debug_populate_devices(void)
+{
+	return bus_for_each_dev(&platform_bus_type, NULL, NULL,
+			pass_iommu_devices);
+}
+
+static int iommu_debug_init_tests(void)
+{
+	iommu_debugfs_top = debugfs_create_dir("iommu", NULL);
+	if (!iommu_debugfs_top) {
+		pr_err("Couldn't create iommu debugfs directory\n");
+		return -ENODEV;
+	}
+	debugfs_tests_dir = debugfs_create_dir("tests",
+					       iommu_debugfs_top);
+	if (!debugfs_tests_dir) {
+		pr_err("Couldn't create iommu/tests debugfs directory\n");
+		return -ENODEV;
+	}
+
+	return iommu_debug_populate_devices();
+}
+
+static void iommu_debug_destroy_tests(void)
+{
+	debugfs_remove_recursive(debugfs_tests_dir);
+}
+#else
+static inline int iommu_debug_init_tests(void) { return 0; }
+static inline void iommu_debug_destroy_tests(void) { }
+#endif
+
+static int __init iommu_debug_init(void)
+{
+	if (iommu_debug_init_tests())
+		return -ENODEV;
+
+	return 0;
+}
+
+static void __exit iommu_debug_exit(void)
+{
+	iommu_debug_destroy_tests();
+}
+
+module_init(iommu_debug_init);
+module_exit(iommu_debug_exit);
+
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/iommu/sunxi-iommu.c b/drivers/iommu/sunxi-iommu.c
new file mode 100644
index 000000000..bf92d95c2
--- /dev/null
+++ b/drivers/iommu/sunxi-iommu.c
@@ -0,0 +1,1860 @@
+/*******************************************************************************
+ * Copyright (C) 2016-2018, Allwinner Technology CO., LTD.
+ * Author: zhuxianbin <zhuxianbin@allwinnertech.com>
+ *
+ * This file is provided under a dual BSD/GPL license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	 See the
+ * GNU General Public License for more details.
+ ******************************************************************************/
+#include <linux/of_iommu.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/of_irq.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/iommu.h>
+#include <linux/dma-mapping.h>
+#include <linux/clk.h>
+#include <linux/dma-iommu.h>
+#include <linux/sizes.h>
+#include <linux/device.h>
+#include <asm/cacheflush.h>
+#include <linux/pm_runtime.h>
+
+#include "sunxi-iommu.h"
+
+#define _max(x, y) (((u64)(x) > (u64)(y)) ? (x) : (y))
+
+#ifdef CONFIG_ARCH_SUN50IW6
+static const char *master[10] = {"DE", "VE_R", "DI", "VE", "CSI", "VP9"};
+#endif
+#ifdef CONFIG_ARCH_SUN50IW3
+static const char *master[10] = {"DE0", "DE1", "VE_R", "VE", "CSI", "VP9"};
+#endif
+#ifdef CONFIG_ARCH_SUN8IW15
+static const char *master[10] = {"DE", "E_EDMA", "E_FE", "VE", "CSI",
+						"G2D", "E_BE", "DEBUG_MODE"};
+#endif
+#ifdef CONFIG_ARCH_SUN50IW9
+static const char *master[10] = {"DE", "DI", "VE_R", "VE", "CSI0", "CSI1",
+						"G2D", "DEBUG_MODE"};
+#endif
+#ifdef CONFIG_ARCH_SUN8IW19
+static const char *master[10] = {"DE", "EISE", "AI", "VE", "CSI", "ISP",
+						"G2D", "DEBUG_MODE"};
+#endif
+#ifdef CONFIG_ARCH_SUN50IW10
+static const char *master[10] = {"DE0", "DE1", "VE", "CSI", "ISP",
+						"G2D", "EINK", "DEBUG_MODE"};
+#endif
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+static const char *master[10] = {"VE", "CSI", "DE", "G2D", "DI",
+						"DEBUG_MODE"};
+#endif
+#ifdef CONFIG_ARCH_SUN50IW12
+static const char *master[10] = {"VE", "VE_R", "TVD_MBUS", "TVD_AXI", "TVCAP",
+						"AV1", "TVFE", "DEBUG_MODE"};
+
+#endif
+
+
+static struct kmem_cache *iopte_cache;
+static struct sunxi_iommu_dev *global_iommu_dev;
+static struct sunxi_iommu_domain *global_sunxi_iommu_domain;
+struct iommu_domain *global_iommu_domain;
+struct sunxi_iommu_owner *global_iommu_owner;
+static struct iommu_group *global_group;
+static bool tlb_init_flag;
+static struct device *dma_dev;
+unsigned int sunxi_iommu_version;
+unsigned int sun50iw10_ic_version;
+
+#define IOMMU_VERSION_V14 0x14
+
+static inline u32 *iopde_offset(u32 *iopd, unsigned int iova)
+{
+	return iopd + IOPDE_INDEX(iova);
+}
+
+static inline u32 *iopte_offset(u32 *ent, unsigned int iova)
+{
+	unsigned long iopte_base = 0;
+
+	if (IOPTE_BASE(*ent) < SUNXI_PHYS_OFFSET)
+		iopte_base = IOPTE_BASE(*ent) + SUNXI_4G_PHYS_OFFSET;
+	else
+		iopte_base = IOPTE_BASE(*ent);
+
+	return (u32 *)__va(iopte_base) + IOPTE_INDEX(iova);
+}
+
+static inline u32 sunxi_iommu_read(struct sunxi_iommu_dev *iommu, u32 offset)
+{
+	return readl(iommu->base + offset);
+}
+
+static inline void sunxi_iommu_write(struct sunxi_iommu_dev *iommu,
+						u32 offset, u32 value)
+{
+	writel(value, iommu->base + offset);
+}
+
+void sunxi_reset_device_iommu(unsigned int master_id)
+{
+	unsigned int regval;
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+
+	regval = sunxi_iommu_read(iommu, IOMMU_RESET_REG);
+	sunxi_iommu_write(iommu, IOMMU_RESET_REG, regval & (~(1 << master_id)));
+	regval = sunxi_iommu_read(iommu, IOMMU_RESET_REG);
+	if (!(regval & ((1 << master_id)))) {
+		sunxi_iommu_write(iommu, IOMMU_RESET_REG, regval | ((1 << master_id)));
+	}
+}
+EXPORT_SYMBOL(sunxi_reset_device_iommu);
+
+void sunxi_enable_device_iommu(unsigned int master_id, bool flag)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	unsigned long mflag;
+
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	if (flag)
+		iommu->bypass &= ~(master_id_bitmap[master_id]);
+	else
+		iommu->bypass |= master_id_bitmap[master_id];
+	sunxi_iommu_write(iommu, IOMMU_BYPASS_REG, iommu->bypass);
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+}
+EXPORT_SYMBOL(sunxi_enable_device_iommu);
+
+static int sunxi_tlb_flush(struct sunxi_iommu_dev *iommu)
+{
+	int ret;
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+	sunxi_iommu_write(iommu, IOMMU_TLB_FLUSH_ENABLE_REG, 0x0003001f);
+#elif IS_ENABLED(CONFIG_ARCH_SUN50IW3) || IS_ENABLED(CONFIG_ARCH_SUN50IW6)
+	sunxi_iommu_write(iommu, IOMMU_TLB_FLUSH_ENABLE_REG, 0x0003003f);
+#else
+	sunxi_iommu_write(iommu, IOMMU_TLB_FLUSH_ENABLE_REG, 0x0003007f);
+#endif
+	ret = sunxi_wait_when(
+		(sunxi_iommu_read(iommu, IOMMU_TLB_FLUSH_ENABLE_REG)), 2);
+	if (ret)
+		dev_err(iommu->dev, "Enable flush all request timed out\n");
+	return ret;
+}
+
+static bool __maybe_unused is_sun50iw10_ic_new(void)
+{
+	if (sun50iw10_ic_version == 0 || sun50iw10_ic_version == 3 ||
+		sun50iw10_ic_version == 4)
+		return false;
+	else
+		return true;
+
+}
+
+static int
+sunxi_tlb_init(struct sunxi_iommu_owner *owner,
+				struct iommu_domain *input_domain)
+{
+	int ret = 0;
+	int iommu_enable = 0;
+	phys_addr_t dte_addr;
+	unsigned long mflag;
+	struct sunxi_iommu_dev *iommu = owner->data;
+	struct sunxi_iommu_domain *sunxi_domain =
+		container_of(input_domain, struct sunxi_iommu_domain, domain);
+
+	/* iommu init */
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	dte_addr = __pa(sunxi_domain->pgtable);
+	sunxi_iommu_write(iommu, IOMMU_TTB_REG, dte_addr);
+	sunxi_iommu_version = sunxi_iommu_read(iommu, IOMMU_VERSION_REG);
+	/* sun8iw15p1: disable preftech on G2D for better performance */
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15)
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x5f);
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW9)
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x0);
+#elif IS_ENABLED(CONFIG_ARCH_SUN50IW10)
+	sun50iw10_ic_version = readl(ioremap(0x03000024, 4)) & 0x00000007;
+	if (is_sun50iw10_ic_new()) {
+		sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x5f);
+	} else {
+		/* disable preftech for tlb invalid mode for unsolved issue for sun50iw10(old)*/
+		sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x0);
+	}
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW20)
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x17);
+#elif IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+	/* sun20iw1p1: disable preftech on G2D/VE for better performance */
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x16);
+#else
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, 0x7f);
+#endif
+	/* disable interrupt of prefetch */
+	sunxi_iommu_write(iommu, IOMMU_INT_ENABLE_REG, 0x3003f);
+	sunxi_iommu_write(iommu, IOMMU_BYPASS_REG, iommu->bypass);
+#if IS_ENABLED(CONFIG_ARCH_SUN50IW9) || IS_ENABLED(CONFIG_ARCH_SUN8IW19)
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_MODE_SEL_REG, 0x1);
+#elif IS_ENABLED(CONFIG_ARCH_SUN50IW10)
+	/* use range(iova_start, iova_end) to invalid TLB */
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_MODE_SEL_REG, 0x1);
+	if (is_sun50iw10_ic_new()) {
+		/* for sun50iw10(new) */
+		/* enable prefetch valid fix mode to avoid prefetching extra invalid TLB or PTW */
+		ret = sunxi_iommu_read(iommu, IOMMU_TLB_PREFETCH_REG);
+		sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, ret | 0x30000);
+	} else {
+		/* for sun50iw10(old) */
+		/* disable prefetch valid fix mode*/
+		ret = sunxi_iommu_read(iommu, IOMMU_TLB_PREFETCH_REG);
+		sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, ret & (0xfffcffff));
+	}
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+	/* use range(iova_start, iova_end) to invalid PTW and TLB */
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_MODE_SEL_REG, 0x1);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_MODE_SEL_REG, 0x1);
+	/* enable prefetch valid fix mode to avoid prefetching extra invalid TLB or PTW */
+	ret = sunxi_iommu_read(iommu, IOMMU_TLB_PREFETCH_REG);
+	sunxi_iommu_write(iommu, IOMMU_TLB_PREFETCH_REG, ret | 0x30000);
+#else
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_MODE_SEL_REG, 0x0);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_MODE_SEL_REG, 0x0);
+#endif
+
+	ret = sunxi_tlb_flush(iommu);
+	if (ret) {
+		dev_err(iommu->dev, "Enable flush all request timed out\n");
+		goto out;
+	}
+	sunxi_iommu_write(iommu, IOMMU_AUTO_GATING_REG, 0x1);
+	sunxi_iommu_write(iommu, IOMMU_ENABLE_REG, IOMMU_ENABLE);
+	iommu_enable = sunxi_iommu_read(iommu, IOMMU_ENABLE_REG);
+	if (iommu_enable != 0x1) {
+		iommu_enable = sunxi_iommu_read(iommu, IOMMU_ENABLE_REG);
+		if (iommu_enable != 0x1) {
+			dev_err(iommu->dev, "iommu enable failed! No iommu in bitfile!\n");
+			ret = -ENODEV;
+			goto out;
+		}
+	}
+	tlb_init_flag = true;
+
+out:
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+	return ret;
+
+}
+
+#if IS_ENABLED(CONFIG_ARCH_SUN50IW9) || IS_ENABLED(CONFIG_ARCH_SUN8IW19) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN50IW10) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+static int sunxi_tlb_invalid(dma_addr_t iova_start, dma_addr_t iova_end)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	int ret = 0;
+	unsigned long mflag;
+
+	pr_debug("iommu: tlb invalid:0x%x-0x%x\n", (unsigned int)iova_start,
+			 (unsigned int)iova_end);
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_START_ADDR_REG, iova_start);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_END_ADDR_REG, iova_end);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	ret = sunxi_wait_when(
+		(sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG)&0x1), 2);
+	if (ret) {
+		dev_err(iommu->dev, "TLB Invalid timed out\n");
+		goto out;
+	}
+
+out:
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+	return ret;
+}
+#else
+static int sunxi_tlb_invalid(dma_addr_t iova, u32 mask)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	int ret = 0;
+	unsigned long mflag;
+
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_REG, iova);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_MASK_REG, mask);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	ret = sunxi_wait_when(
+		(sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG)&0x1), 2);
+	if (ret) {
+		dev_err(iommu->dev, "TLB Invalid timed out\n");
+		goto out;
+	}
+
+out:
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+	return ret;
+}
+#endif
+
+/* sun8iw20/sun20iw1/sun50iw12/sun50iw10(new) iommu version >= 0x14: use start/end to invalid ptw*/
+static int __maybe_unused sunxi_ptw_cache_invalid_new(dma_addr_t iova_start, dma_addr_t iova_end)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	int ret = 0;
+	unsigned long mflag;
+
+	pr_debug("iommu: ptw invalid:0x%x-0x%x\n", (unsigned int)iova_start,
+			 (unsigned int)iova_end);
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_START_ADDR_REG, iova_start);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_END_ADDR_REG, iova_end);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	ret = sunxi_wait_when(
+		(sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG)&0x1), 2);
+	if (ret) {
+		dev_err(iommu->dev, "PTW cache invalid timed out\n");
+		goto out;
+	}
+
+out:
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+	return ret;
+}
+
+/* old platform(including sun50iw10(old)) iommu version < 0x14*/
+static int __maybe_unused sunxi_ptw_cache_invalid(dma_addr_t iova)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	int ret = 0;
+	unsigned long mflag;
+
+	pr_debug("iommu: ptw invalid:0x%x\n", (unsigned int)iova);
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG, iova);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	ret = sunxi_wait_when(
+		(sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG)&0x1), 2);
+	if (ret) {
+		dev_err(iommu->dev, "PTW cache invalid timed out\n");
+		goto out;
+	}
+
+out:
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+	return ret;
+}
+
+static int sunxi_alloc_iopte(u32 *sent, int prot)
+{
+	u32 *pent;
+	u32 flags = 0;
+
+	flags |= (prot & IOMMU_READ) ? DENT_READABLE : 0;
+	flags |= (prot & IOMMU_WRITE) ? DENT_WRITABLE : 0;
+
+	pent = kmem_cache_zalloc(iopte_cache, GFP_ATOMIC);
+	WARN_ON((unsigned long)pent & (PT_SIZE - 1));
+	if (!pent) {
+		pr_err("%s, %d, malloc failed!\n", __func__, __LINE__);
+		return 0;
+	}
+	dma_sync_single_for_cpu(dma_dev, virt_to_phys(sent), sizeof(*sent), DMA_TO_DEVICE);
+	*sent = __pa(pent) | DENT_VALID;
+	dma_sync_single_for_device(dma_dev, virt_to_phys(sent), sizeof(*sent), DMA_TO_DEVICE);
+
+	return 1;
+}
+
+static void sunxi_free_iopte(u32 *pent)
+{
+	kmem_cache_free(iopte_cache, pent);
+}
+
+#if IS_ENABLED(CONFIG_ARCH_SUN50IW6) || IS_ENABLED(CONFIG_ARCH_SUN50IW3) || IS_ENABLED(CONFIG_ARCH_SUN8IW15)
+void sunxi_zap_tlb(unsigned long iova, size_t size)
+{
+	sunxi_tlb_invalid(iova, (u32)IOMMU_PT_MASK);
+	sunxi_tlb_invalid(iova + SPAGE_SIZE, (u32)IOMMU_PT_MASK);
+	sunxi_tlb_invalid(iova + size, (u32)IOMMU_PT_MASK);
+	sunxi_tlb_invalid(iova + size + SPAGE_SIZE, (u32)IOMMU_PT_MASK);
+	sunxi_ptw_cache_invalid(iova);
+	sunxi_ptw_cache_invalid(iova + SPD_SIZE);
+	sunxi_ptw_cache_invalid(iova + size);
+	sunxi_ptw_cache_invalid(iova + size + SPD_SIZE);
+}
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+static inline void sunxi_zap_tlb(unsigned long iova, size_t size)
+{
+	sunxi_tlb_invalid(iova, iova + 2 * SPAGE_SIZE);
+	sunxi_tlb_invalid(iova + size - SPAGE_SIZE, iova + size + 8 * SPAGE_SIZE);
+	sunxi_ptw_cache_invalid_new(iova, iova + SPD_SIZE);
+	sunxi_ptw_cache_invalid_new(iova + size - SPD_SIZE, iova + size);
+}
+#else
+static inline void sunxi_zap_tlb(unsigned long iova, size_t size)
+{
+	sunxi_tlb_invalid(iova, iova + 2 * SPAGE_SIZE);
+	sunxi_tlb_invalid(iova + size - SPAGE_SIZE, iova + size + 8 * SPAGE_SIZE);
+	sunxi_ptw_cache_invalid(iova);
+	sunxi_ptw_cache_invalid(iova + size);
+	/* new version of iommu (>= 0x14) and sun50iw10(new) will not pretetch any extra invalid TLB or PTW ,
+	   so no need to walk around(sunxi_zap_tlb) when unmap, still need it in map */
+	if (sunxi_iommu_version >= IOMMU_VERSION_V14 || is_sun50iw10_ic_new())
+		return ;
+
+	sunxi_ptw_cache_invalid(iova + SPD_SIZE);
+	sunxi_ptw_cache_invalid(iova + size + SPD_SIZE);
+	sunxi_ptw_cache_invalid(iova + size + 2 * SPD_SIZE);
+}
+#endif
+
+static inline u32 sunxi_mk_pte(u32 page, int prot)
+{
+	u32 flags = 0;
+
+	flags |= (prot & IOMMU_READ) ? SUNXI_PTE_PAGE_READABLE : 0;
+	flags |= (prot & IOMMU_WRITE) ? SUNXI_PTE_PAGE_WRITABLE : 0;
+	page &= IOMMU_PT_MASK;
+	return page | flags | SUNXI_PTE_PAGE_VALID;
+}
+
+
+static int sunxi_iommu_map(struct iommu_domain *domain, unsigned long iova,
+	   phys_addr_t paddr, size_t size, int prot)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+		container_of(domain, struct sunxi_iommu_domain, domain);
+	size_t iova_start, iova_end, paddr_start, s_iova_start;
+	u32 *dent, *pent;
+	int i, j;
+	int flush_count = 0;
+
+	WARN_ON(sunxi_domain->pgtable == NULL);
+	iova_start = iova & IOMMU_PT_MASK;
+	paddr_start = paddr & IOMMU_PT_MASK;
+	iova_end = SPAGE_ALIGN(iova+size) - SPAGE_SIZE;
+	s_iova_start = iova_start;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+	if (IOPDE_INDEX(iova_start) == IOPDE_INDEX(iova_end)) {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (!IS_VALID(*dent))
+			sunxi_alloc_iopte(dent, prot);
+		for (flush_count = 0; iova_start <= iova_end;
+			iova_start += SPAGE_SIZE, paddr_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			WARN_ON(*pent);
+			*pent = sunxi_mk_pte(paddr_start, prot);
+			++flush_count;
+		}
+		dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+			flush_count << 2, DMA_TO_DEVICE);
+		goto out;
+	} else {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (!IS_VALID(*dent))
+			sunxi_alloc_iopte(dent, prot);
+		for (flush_count = 0; iova_start < SPDE_ALIGN(s_iova_start + 1);
+			iova_start += SPAGE_SIZE, paddr_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			WARN_ON(*pent);
+			*pent = sunxi_mk_pte(paddr_start, prot);
+			++flush_count;
+		}
+		dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+			flush_count << 2, DMA_TO_DEVICE);
+	}
+	if (IOPDE_INDEX(iova_start) < IOPDE_INDEX(iova_end)) {
+		for (i = IOPDE_INDEX(iova_start); i < IOPDE_INDEX(iova_end);
+			i++, iova_start += SPD_SIZE, paddr_start += SPD_SIZE) {
+			dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+			if (!IS_VALID(*dent))
+				sunxi_alloc_iopte(dent, prot);
+			pent = iopte_offset(dent, iova_start);
+			dma_sync_single_for_cpu(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+			for (j = 0; j < NUM_ENTRIES_PTE; j++)
+				*(pent + j) =
+			sunxi_mk_pte(paddr_start + (j * SPAGE_SIZE), prot);
+			dma_sync_single_for_device(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+		}
+	}
+
+	s_iova_start = iova_start;
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (!IS_VALID(*dent))
+		sunxi_alloc_iopte(dent, prot);
+	for (flush_count = 0; iova_start <= iova_end;
+			iova_start += SPAGE_SIZE, paddr_start += SPAGE_SIZE) {
+		pent = iopte_offset(dent, iova_start);
+		WARN_ON(*pent);
+		*pent = sunxi_mk_pte(paddr_start, prot);
+		++flush_count;
+	}
+	dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+			flush_count << 2, DMA_TO_DEVICE);
+out:
+	if (!(prot & (1 << 16)))
+		sunxi_zap_tlb(iova, size);
+	mutex_unlock(&sunxi_domain->dt_lock);
+
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_ARCH_SUN50IW9) || IS_ENABLED(CONFIG_ARCH_SUN8IW19) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN50IW10)
+static size_t sunxi_iommu_unmap(struct iommu_domain *domain, unsigned long iova,
+		 size_t size, struct iommu_iotlb_gather *gather)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+			container_of(domain, struct sunxi_iommu_domain, domain);
+	size_t iova_start, iova_end;
+	size_t s_iova_start;
+	u32 *dent, *pent;
+	int i;
+	int flush_count = 0;
+
+	WARN_ON(sunxi_domain->pgtable == NULL);
+	iova_start = iova & IOMMU_PT_MASK;
+	iova_end = SPAGE_ALIGN(iova + size) - SPAGE_SIZE;
+
+	gather->start = iova_start;
+	gather->end = iova_end;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+
+	/* Invalid TLB */
+	sunxi_tlb_invalid(iova_start, iova_end);
+
+	/* Invalid PTW and clear iommu pagetable pde and pte */
+	/* 1
+	 * if the iova_start and iova_end are between PD_SIZE(1M)
+	 * for example the iova is between: 0x100000--0x120000 */
+	s_iova_start = iova_start;
+	if (IOPDE_INDEX(iova_start) == IOPDE_INDEX(iova_end)) {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (IS_VALID(*dent)) {
+			for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+				pent = iopte_offset(dent, iova_start);
+				*pent = 0;
+				++flush_count;
+			}
+			dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+					flush_count << 2, DMA_TO_DEVICE);
+			/*invalid ptwcache*/
+			sunxi_ptw_cache_invalid(s_iova_start);
+		}
+		goto done;
+	}
+	/* 2
+	 * if the iova_start and iova_end are not between PD_SIZE(1M)
+	 * for example the iova is between: 0x120000--0x540000
+	 */
+
+	/* 2.1. Handle the iova between:0x120000--0x1ff000 */
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (IS_VALID(*dent)) {
+		for (flush_count = 0;
+			iova_start < SPDE_ALIGN(s_iova_start + 1);
+					iova_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			*pent = 0;
+			++flush_count;
+		}
+		dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+				flush_count << 2, DMA_TO_DEVICE);
+		/*invalid ptwcache*/
+		sunxi_ptw_cache_invalid(s_iova_start);
+	}
+
+	/* 2.2. Handle the iova between:0x200000--0x500000 */
+	if (IOPDE_INDEX(iova_start) < IOPDE_INDEX(iova_end)) {
+		for (i = IOPDE_INDEX(iova_start); i < IOPDE_INDEX(iova_end);
+			i++, iova_start += SPD_SIZE) {
+			dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+			if (IS_VALID(*dent)) {
+				pent = iopte_offset(dent, iova_start);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				memset(pent, 0, PT_SIZE);
+				dma_sync_single_for_device(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				*dent = 0;
+				dma_sync_single_for_device(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				sunxi_ptw_cache_invalid(iova_start);
+				sunxi_free_iopte(pent);
+			}
+		}
+	}
+	/* 2.3. Handle the iova between:0x500000--0x520000 */
+	s_iova_start = iova_start;
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (IS_VALID(*dent)) {
+		for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			*pent = 0;
+			++flush_count;
+		}
+		dma_sync_single_for_cpu(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+				flush_count << 2, DMA_TO_DEVICE);
+		sunxi_ptw_cache_invalid(s_iova_start);
+	}
+
+done:
+	if (sunxi_iommu_version < IOMMU_VERSION_V14 || !is_sun50iw10_ic_new())
+		sunxi_zap_tlb(iova, size);
+	mutex_unlock(&sunxi_domain->dt_lock);
+
+	return size;
+}
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+static size_t sunxi_iommu_unmap(struct iommu_domain *domain, unsigned long iova,
+		 size_t size, struct iommu_iotlb_gather *gather)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+			container_of(domain, struct sunxi_iommu_domain, domain);
+	size_t iova_start, iova_end;
+	size_t s_iova_start;
+	u32 *dent, *pent;
+	int i;
+	int flush_count = 0;
+
+	WARN_ON(sunxi_domain->pgtable == NULL);
+	iova_start = iova & IOMMU_PT_MASK;
+	iova_end = SPAGE_ALIGN(iova + size) - SPAGE_SIZE;
+
+	gather->start = iova_start;
+	gather->end = iova_end;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+
+	/* Invalid TLB and PTW :*/
+	sunxi_tlb_invalid(iova_start, iova_end);
+	sunxi_ptw_cache_invalid_new(iova_start, iova_end);
+
+	/* Invalid PTW and clear iommu pagetable pde and pte */
+	/* 1
+	 * if the iova_start and iova_end are between PD_SIZE(1M)
+	 * for example the iova is between: 0x100000--0x200000 */
+	s_iova_start = iova_start;
+	if (IOPDE_INDEX(iova_start) == IOPDE_INDEX(iova_end)) {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (IS_VALID(*dent)) {
+			for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+				pent = iopte_offset(dent, iova_start);
+				*pent = 0;
+				++flush_count;
+			}
+			dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+					flush_count << 2, DMA_TO_DEVICE);
+		}
+		goto done;
+	}
+	/* 2
+	 * if the iova_start and iova_end are not between PD_SIZE(1M)
+	 * for example the iova is between: 0x120000--0x540000
+	 */
+	/* 2.1. Handle the iova between:0x120000--0x1ff000 */
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (IS_VALID(*dent)) {
+		for (flush_count = 0;
+			iova_start < SPDE_ALIGN(s_iova_start + 1);
+					iova_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			*pent = 0;
+			++flush_count;
+		}
+		dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+				flush_count << 2, DMA_TO_DEVICE);
+	}
+	/* 2.2. Handle the iova between:0x200000--0x500000 */
+	if (IOPDE_INDEX(iova_start) < IOPDE_INDEX(iova_end)) {
+		for (i = IOPDE_INDEX(iova_start); i < IOPDE_INDEX(iova_end);
+			i++, iova_start += SPD_SIZE) {
+			dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+			if (IS_VALID(*dent)) {
+				pent = iopte_offset(dent, iova_start);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				memset(pent, 0, PT_SIZE);
+				dma_sync_single_for_device(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				*dent = 0;
+				dma_sync_single_for_device(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				sunxi_free_iopte(pent);
+			}
+		}
+	}
+	/* 2.3. Handle the iova between:0x500000--0x520000 */
+	s_iova_start = iova_start;
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (IS_VALID(*dent)) {
+		for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			*pent = 0;
+			++flush_count;
+		}
+		dma_sync_single_for_cpu(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+				flush_count << 2, DMA_TO_DEVICE);
+	}
+
+done:
+	mutex_unlock(&sunxi_domain->dt_lock);
+
+	return size;
+}
+#else
+static size_t sunxi_iommu_unmap(struct iommu_domain *domain, unsigned long iova,
+		 size_t size, struct iommu_iotlb_gather *gather)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+			container_of(domain, struct sunxi_iommu_domain, domain);
+	size_t iova_start, iova_end, s_iova_start;
+	u32 *dent, *pent;
+	int i;
+	int flush_count = 0;
+
+	WARN_ON(sunxi_domain->pgtable == NULL);
+	iova_start = iova & IOMMU_PT_MASK;
+	iova_end = SPAGE_ALIGN(iova+size) - SPAGE_SIZE;
+	s_iova_start = iova_start;
+
+	gather->start = iova_start;
+	gather->end = iova_end;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+
+	if (IOPDE_INDEX(iova_start) == IOPDE_INDEX(iova_end)) {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (IS_VALID(*dent)) {
+			for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+				pent = iopte_offset(dent, iova_start);
+				*pent = 0;
+				sunxi_tlb_invalid(iova_start,
+							(u32)IOMMU_PT_MASK);
+				++flush_count;
+			}
+			dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+					flush_count << 2, DMA_TO_DEVICE);
+			/*invalid ptwcache*/
+			sunxi_ptw_cache_invalid(s_iova_start);
+		}
+		goto out;
+	} else {
+		dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+		if (IS_VALID(*dent)) {
+			for (flush_count = 0;
+				iova_start < SPDE_ALIGN(s_iova_start + 1);
+						iova_start += SPAGE_SIZE) {
+				pent = iopte_offset(dent, iova_start);
+				*pent = 0;
+				sunxi_tlb_invalid(iova_start,
+							(u32)IOMMU_PT_MASK);
+				++flush_count;
+			}
+			dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+					flush_count << 2, DMA_TO_DEVICE);
+			/*invalid ptwcache*/
+			sunxi_ptw_cache_invalid(s_iova_start);
+		}
+	}
+	if (IOPDE_INDEX(iova_start) < IOPDE_INDEX(iova_end)) {
+		for (i = IOPDE_INDEX(iova_start); i < IOPDE_INDEX(iova_end);
+			i++, iova_start += SPD_SIZE) {
+			dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+			if (IS_VALID(*dent)) {
+				pent = iopte_offset(dent, iova_start);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				memset(pent, 0, PT_SIZE);
+				dma_sync_single_for_device(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+				sunxi_tlb_invalid(iova_start,
+							(u32)IOMMU_PD_MASK);
+				dma_sync_single_for_cpu(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				*dent = 0;
+				dma_sync_single_for_device(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+				sunxi_ptw_cache_invalid(iova_start);
+				sunxi_free_iopte(pent);
+			}
+		}
+	}
+	s_iova_start = iova_start;
+	dent = iopde_offset(sunxi_domain->pgtable, iova_start);
+	if (IS_VALID(*dent)) {
+		for (flush_count = 0; iova_start <= iova_end;
+						iova_start += SPAGE_SIZE) {
+			pent = iopte_offset(dent, iova_start);
+			*pent = 0;
+			sunxi_tlb_invalid(iova_start, (u32)IOMMU_PT_MASK);
+			++flush_count;
+		}
+		dma_sync_single_for_device(dma_dev, virt_to_phys(iopte_offset(dent, s_iova_start)),
+				flush_count << 2, DMA_TO_DEVICE);
+		sunxi_ptw_cache_invalid(s_iova_start);
+	}
+out:
+	mutex_unlock(&sunxi_domain->dt_lock);
+
+	return size;
+}
+#endif
+
+void sunxi_iommu_iotlb_sync(struct iommu_domain *domain,
+			struct iommu_iotlb_gather *gather)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+		container_of(domain, struct sunxi_iommu_domain, domain);
+	unsigned long iova_start = gather->start & IOMMU_PT_MASK;
+	unsigned long iova_end = SPAGE_ALIGN(gather->end);
+
+	size_t size = iova_end - iova_start;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+	sunxi_zap_tlb(iova_start, size);
+	mutex_unlock(&sunxi_domain->dt_lock);
+
+	return;
+}
+
+static phys_addr_t
+sunxi_iommu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+		container_of(domain, struct sunxi_iommu_domain, domain);
+	u32 *dent, *pent;
+	phys_addr_t ret = 0;
+
+
+	WARN_ON(sunxi_domain->pgtable == NULL);
+	mutex_lock(&sunxi_domain->dt_lock);
+	dent = iopde_offset(sunxi_domain->pgtable, iova);
+	if (IS_VALID(*dent)) {
+		pent = iopte_offset(dent, iova);
+		ret = IOPTE_TO_PFN(pent) + IOVA_PAGE_OFT(iova);
+	}
+	if (ret < SUNXI_PHYS_OFFSET)
+		ret += SUNXI_4G_PHYS_OFFSET;
+	mutex_unlock(&sunxi_domain->dt_lock);
+	return ret;
+}
+
+static struct iommu_domain *sunxi_iommu_domain_alloc(unsigned type)
+{
+	struct sunxi_iommu_domain *sunxi_domain;
+
+	if (type != IOMMU_DOMAIN_DMA && type != IOMMU_DOMAIN_UNMANAGED)
+		return NULL;
+
+	/* we just use one domain */
+	if (global_sunxi_iommu_domain)
+		return &global_sunxi_iommu_domain->domain;
+
+	sunxi_domain = kzalloc(sizeof(*sunxi_domain), GFP_KERNEL);
+
+	if (!sunxi_domain)
+		return NULL;
+
+	sunxi_domain->pgtable = (unsigned int *)__get_free_pages(
+				GFP_KERNEL, get_order(PD_SIZE));
+	if (!sunxi_domain->pgtable) {
+		pr_err("sunxi domain get pgtable failed\n");
+		goto err_page;
+	}
+
+	sunxi_domain->sg_buffer = (unsigned int *)__get_free_pages(
+				GFP_KERNEL, get_order(MAX_SG_TABLE_SIZE));
+	if (!sunxi_domain->sg_buffer) {
+		pr_err("sunxi domain get sg_buffer failed\n");
+		goto err_sg_buffer;
+	}
+
+	if (type == IOMMU_DOMAIN_DMA &&
+				iommu_get_dma_cookie(&sunxi_domain->domain)) {
+		pr_err("sunxi domain get dma cookie failed\n");
+		goto err_dma_cookie;
+	}
+
+	memset(sunxi_domain->pgtable, 0, PD_SIZE);
+	sunxi_domain->domain.geometry.aperture_start = 0;
+	sunxi_domain->domain.geometry.aperture_end	 = (1ULL << 32)-1;
+	sunxi_domain->domain.geometry.force_aperture = true;
+	mutex_init(&sunxi_domain->dt_lock);
+	global_sunxi_iommu_domain = sunxi_domain;
+	global_iommu_domain = &sunxi_domain->domain;
+
+	return &sunxi_domain->domain;
+
+err_dma_cookie:
+err_sg_buffer:
+	free_pages((unsigned long)sunxi_domain->pgtable, get_order(PD_SIZE));
+	sunxi_domain->pgtable = NULL;
+err_page:
+	kfree(sunxi_domain);
+	return NULL;
+}
+
+static void sunxi_iommu_domain_free(struct iommu_domain *domain)
+{
+	struct sunxi_iommu_domain *sunxi_domain =
+		container_of(domain, struct sunxi_iommu_domain, domain);
+	int i = 0;
+	size_t iova;
+	u32 *dent, *pent;
+
+	mutex_lock(&sunxi_domain->dt_lock);
+	for (i = 0; i < NUM_ENTRIES_PDE; ++i) {
+		dent = sunxi_domain->pgtable + i;
+		iova = i << IOMMU_PD_SHIFT;
+		if (IS_VALID(*dent)) {
+			pent = iopte_offset(dent, iova);
+			dma_sync_single_for_cpu(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+			memset(pent, 0, PT_SIZE);
+			dma_sync_single_for_device(dma_dev, virt_to_phys(pent), PT_SIZE, DMA_TO_DEVICE);
+			dma_sync_single_for_cpu(dma_dev, virt_to_phys(dent), PT_SIZE, DMA_TO_DEVICE);
+			*dent = 0;
+			dma_sync_single_for_device(dma_dev, virt_to_phys(dent), sizeof(*dent), DMA_TO_DEVICE);
+			sunxi_free_iopte(pent);
+		}
+	}
+	sunxi_tlb_flush(global_iommu_dev);
+	mutex_unlock(&sunxi_domain->dt_lock);
+	free_pages((unsigned long)sunxi_domain->pgtable, get_order(PD_SIZE));
+	sunxi_domain->pgtable = NULL;
+	free_pages((unsigned long)sunxi_domain->sg_buffer,
+						get_order(MAX_SG_TABLE_SIZE));
+	sunxi_domain->sg_buffer = NULL;
+	iommu_put_dma_cookie(domain);
+	kfree(sunxi_domain);
+}
+
+static int
+sunxi_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)
+{
+	int ret = 0;
+
+#if IS_ENABLED(CONFIG_IOMMU_DEBUG) && !IS_ENABLED(CONFIG_ARM_DMA_USE_IOMMU)
+//	do_iommu_attach(dev, NULL, 0, SZ_1G * 4UL);
+#endif
+	return ret;
+}
+
+static void
+sunxi_iommu_detach_dev(struct iommu_domain *domain, struct device *dev)
+{
+		return;
+}
+
+static int sunxi_iommu_add_device(struct device *dev)
+{
+	struct iommu_group *group;
+	struct sunxi_iommu_owner *owner = dev->archdata.iommu;
+	int ret = 0;
+
+	if (!dev->archdata.iommu) /* Not a iommu client device */
+		return -ENODEV;
+
+	group = iommu_group_get_for_dev(dev);
+
+	if (IS_ERR(group)) {
+		ret = PTR_ERR(group);
+		pr_err("sunxi iommu get group failed, err_no:%d\n", ret);
+	}
+	if (!tlb_init_flag) {
+		global_iommu_owner = owner;
+		ret = sunxi_tlb_init(owner, &global_sunxi_iommu_domain->domain);
+		if (ret)
+			pr_err("sunxi iommu init tlb failed\n");
+	}
+	sunxi_enable_device_iommu(owner->tlbid, owner->flag);
+
+	iommu_group_put(group);
+
+	return ret;
+}
+
+static void sunxi_iommu_remove_device(struct device *dev)
+{
+	struct sunxi_iommu_owner *owner = dev->archdata.iommu;
+
+	if (!owner)
+		return;
+
+	sunxi_enable_device_iommu(owner->tlbid, false);
+	dev->iommu_group = NULL;
+	devm_kfree(dev, dev->dma_parms);
+	dev->dma_parms = NULL;
+	kfree(owner);
+	owner = NULL;
+	dev->archdata.iommu = NULL;
+	iommu_group_remove_device(dev);
+}
+
+/* set dma params for master devices */
+int sunxi_iommu_set_dma_parms(struct notifier_block *nb,
+				unsigned long action, void *data)
+{
+	struct device *dev = data;
+
+	if (action != IOMMU_GROUP_NOTIFY_BIND_DRIVER)
+		return 0;
+
+	dev->dma_parms = devm_kzalloc(dev, sizeof(*dev->dma_parms), GFP_KERNEL);
+	if (!dev->dma_parms)
+		return -ENOMEM;
+	dma_set_max_seg_size(dev, DMA_BIT_MASK(32));
+
+	return 0;
+}
+
+struct iommu_group *sunxi_iommu_device_group(struct device *dev)
+{
+	struct iommu_group *group;
+	struct notifier_block *nb;
+
+	if (!global_group) {
+		nb = kzalloc(sizeof(*nb), GFP_KERNEL);
+		if (!nb)
+			return ERR_PTR(-ENOMEM);
+
+		global_group = iommu_group_alloc();
+		if (IS_ERR(global_group)) {
+			pr_err("sunxi iommu alloc group failed\n");
+			goto err_group_alloc;
+		}
+
+		nb->notifier_call = sunxi_iommu_set_dma_parms;
+		if (iommu_group_register_notifier(global_group, nb)) {
+			pr_err("sunxi iommu group register notifier failed!\n");
+			goto err_notifier;
+		}
+
+	}
+	group = global_group;
+	return group;
+
+err_notifier:
+err_group_alloc:
+	kfree(nb);
+	return ERR_PTR(-EBUSY);
+}
+
+static int
+sunxi_iommu_of_xlate(struct device *dev, struct of_phandle_args *args)
+{
+	struct sunxi_iommu_owner *owner = dev->archdata.iommu;
+	struct platform_device *sysmmu = of_find_device_by_node(args->np);
+	struct sunxi_iommu_dev *data;
+
+	if (!sysmmu)
+		return -ENODEV;
+
+	data = platform_get_drvdata(sysmmu);
+	if (data == NULL)
+		return -ENODEV;
+
+	if (!owner) {
+		owner = kzalloc(sizeof(*owner), GFP_KERNEL);
+		if (!owner)
+			return -ENOMEM;
+		owner->tlbid = args->args[0];
+		owner->flag = args->args[1];
+		owner->data = data;
+		owner->dev = dev;
+		dev->archdata.iommu = owner;
+	}
+
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) || IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+void sunxi_set_debug_mode(void)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x80000000);
+}
+
+void sunxi_set_prefetch_mode(void)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x00000000);
+}
+
+#else
+void sunxi_set_debug_mode(void){ return; }
+void sunxi_set_prefetch_mode(void){ return; }
+#endif
+
+int sunxi_iova_test_write(dma_addr_t iova, u32 val)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	int retval;
+
+	sunxi_iommu_write(iommu, IOMMU_VA_REG, iova);
+	sunxi_iommu_write(iommu, IOMMU_VA_DATA_REG, val);
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x80000100);
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x80000101);
+	retval = sunxi_wait_when((sunxi_iommu_read(iommu,
+				IOMMU_VA_CONFIG_REG) & 0x1), 1);
+	if (retval)
+		dev_err(iommu->dev,
+			"write VA address request timed out\n");
+	return retval;
+}
+
+unsigned long sunxi_iova_test_read(dma_addr_t iova)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	unsigned long retval;
+
+	sunxi_iommu_write(iommu, IOMMU_VA_REG, iova);
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x80000000);
+	sunxi_iommu_write(iommu,
+			IOMMU_VA_CONFIG_REG, 0x80000001);
+	retval = sunxi_wait_when((sunxi_iommu_read(iommu,
+				IOMMU_VA_CONFIG_REG) & 0x1), 1);
+	if (retval) {
+		dev_err(iommu->dev,
+			"read VA address request timed out\n");
+		retval = 0;
+		goto out;
+	}
+	retval = sunxi_iommu_read(iommu,
+			IOMMU_VA_DATA_REG);
+out:
+	return retval;
+}
+
+static int sunxi_iova_invalid_helper(unsigned long iova)
+{
+	struct sunxi_iommu_domain *sunxi_domain = global_sunxi_iommu_domain;
+	u32 *pte_addr, *dte_addr;
+
+	dte_addr = iopde_offset(sunxi_domain->pgtable, iova);
+	if ((*dte_addr & 0x3) != 0x1) {
+		pr_err("0x%lx is not mapped!\n", iova);
+		return 1;
+	}
+	pte_addr = iopte_offset(dte_addr, iova);
+	if ((*pte_addr & 0x2) == 0) {
+		pr_err("0x%lx is not mapped!\n", iova);
+		return 1;
+	}
+	pr_err("0x%lx is mapped!\n", iova);
+	return 0;
+}
+
+static irqreturn_t sunxi_iommu_irq(int irq, void *dev_id)
+{
+
+	u32 inter_status_reg = 0;
+	u32 addr_reg = 0;
+	u32	int_masterid_bitmap = 0;
+	u32	data_reg = 0;
+	u32	l1_pgint_reg = 0;
+	u32	l2_pgint_reg = 0;
+	u32	master_id = 0;
+	unsigned long mflag;
+	struct sunxi_iommu_dev *iommu = dev_id;
+
+	spin_lock_irqsave(&iommu->iommu_lock, mflag);
+	inter_status_reg = sunxi_iommu_read(iommu, IOMMU_INT_STA_REG) & 0x3ffff;
+	l1_pgint_reg = sunxi_iommu_read(iommu, IOMMU_L1PG_INT_REG);
+	l2_pgint_reg = sunxi_iommu_read(iommu, IOMMU_L2PG_INT_REG);
+	int_masterid_bitmap = inter_status_reg | l1_pgint_reg | l2_pgint_reg;
+
+	if (inter_status_reg & MICRO_TLB0_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[0]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG0);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG0);
+	} else if (inter_status_reg & MICRO_TLB1_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[1]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG1);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG1);
+	} else if (inter_status_reg & MICRO_TLB2_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[2]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG2);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG2);
+	} else if (inter_status_reg & MICRO_TLB3_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[3]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG3);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG3);
+	} else if (inter_status_reg & MICRO_TLB4_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[4]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG4);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG4);
+	} else if (inter_status_reg & MICRO_TLB5_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[5]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG5);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG5);
+	}
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) || IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+	else if (inter_status_reg & MICRO_TLB6_INVALID_INTER_MASK) {
+		pr_err("%s Invalid Authority\n", master[6]);
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG6);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG6);
+	} else if (inter_status_reg & L1_PAGETABLE_INVALID_INTER_MASK) {
+		/*It's OK to prefetch an invalid pagetable,no need to print msg for debug.*/
+		if (!(int_masterid_bitmap & (1U << 31)))
+			pr_err("L1 PageTable Invalid\n");
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG7);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG7);
+	} else if (inter_status_reg & L2_PAGETABLE_INVALID_INTER_MASK) {
+		if (!(int_masterid_bitmap & (1U << 31)))
+			pr_err("L2 PageTable Invalid\n");
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG8);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG8);
+	}
+#else
+	else if (inter_status_reg & L1_PAGETABLE_INVALID_INTER_MASK) {
+		pr_err("L1 PageTable Invalid\n");
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG6);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG6);
+	} else if (inter_status_reg & L2_PAGETABLE_INVALID_INTER_MASK) {
+		pr_err("L2 PageTable Invalid\n");
+		addr_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_ADDR_REG7);
+		data_reg = sunxi_iommu_read(iommu, IOMMU_INT_ERR_DATA_REG7);
+	}
+#endif
+	else
+		pr_err("sunxi iommu int error!!!\n");
+
+	if (!(int_masterid_bitmap & (1U << 31))) {
+		if (sunxi_iova_invalid_helper(addr_reg)) {
+			int_masterid_bitmap &= 0xffff;
+			master_id = __ffs(int_masterid_bitmap);
+		}
+		pr_err("%s invalid address: 0x%x, data:0x%x, id:0x%x\n",
+			master[master_id], addr_reg, data_reg,
+				int_masterid_bitmap);
+	}
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN50IW10)
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_START_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_END_ADDR_REG, addr_reg + 4 * SPAGE_SIZE);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG,
+		addr_reg + 0x200000);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG) & 0x1)
+		;
+#elif IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN20IW1)
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_START_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_END_ADDR_REG, addr_reg + 4 * SPAGE_SIZE);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_START_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_END_ADDR_REG, addr_reg + 2 * SPD_SIZE);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG) & 0x1)
+		;
+#else
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_MASK_REG,
+		(u32)IOMMU_PT_MASK);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_REG,
+		addr_reg + 0x2000);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ADDR_MASK_REG,
+		(u32)IOMMU_PT_MASK);
+	sunxi_iommu_write(iommu, IOMMU_TLB_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_TLB_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG, addr_reg);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG) & 0x1)
+		;
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ADDR_REG,
+		addr_reg + 0x200000);
+	sunxi_iommu_write(iommu, IOMMU_PC_IVLD_ENABLE_REG, 0x1);
+	while (sunxi_iommu_read(iommu, IOMMU_PC_IVLD_ENABLE_REG) & 0x1)
+		;
+#endif
+
+	sunxi_iommu_write(iommu, IOMMU_INT_CLR_REG, inter_status_reg);
+	inter_status_reg |= (l1_pgint_reg | l2_pgint_reg);
+	inter_status_reg &= 0xffff;
+	sunxi_iommu_write(iommu, IOMMU_RESET_REG, ~inter_status_reg);
+	sunxi_iommu_write(iommu, IOMMU_RESET_REG, 0xffffffff);
+	spin_unlock_irqrestore(&iommu->iommu_lock, mflag);
+
+	return IRQ_HANDLED;
+}
+
+static ssize_t sunxi_iommu_enable_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	u32 data;
+
+	spin_lock(&iommu->iommu_lock);
+	data = sunxi_iommu_read(iommu, IOMMU_PMU_ENABLE_REG);
+	spin_unlock(&iommu->iommu_lock);
+	return snprintf(buf, PAGE_SIZE,
+		"enable = %d\n", data & 0x1 ? 1 : 0);
+}
+
+static ssize_t sunxi_iommu_enable_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	unsigned long val;
+	u32 data;
+	int retval;
+
+	if (kstrtoul(buf, 0, &val))
+		return -EINVAL;
+
+	if (val) {
+		spin_lock(&iommu->iommu_lock);
+		data = sunxi_iommu_read(iommu, IOMMU_PMU_ENABLE_REG);
+		sunxi_iommu_write(iommu, IOMMU_PMU_ENABLE_REG, data | 0x1);
+		data = sunxi_iommu_read(iommu, IOMMU_PMU_CLR_REG);
+		sunxi_iommu_write(iommu, IOMMU_PMU_CLR_REG, data | 0x1);
+		retval = sunxi_wait_when((sunxi_iommu_read(iommu,
+				IOMMU_PMU_CLR_REG) & 0x1), 1);
+		if (retval)
+			dev_err(iommu->dev, "Clear PMU Count timed out\n");
+		spin_unlock(&iommu->iommu_lock);
+	} else {
+		spin_lock(&iommu->iommu_lock);
+		data = sunxi_iommu_read(iommu, IOMMU_PMU_CLR_REG);
+		sunxi_iommu_write(iommu, IOMMU_PMU_CLR_REG, data | 0x1);
+		retval = sunxi_wait_when((sunxi_iommu_read(iommu,
+				IOMMU_PMU_CLR_REG) & 0x1), 1);
+		if (retval)
+			dev_err(iommu->dev, "Clear PMU Count timed out\n");
+		data = sunxi_iommu_read(iommu, IOMMU_PMU_ENABLE_REG);
+		sunxi_iommu_write(iommu, IOMMU_PMU_ENABLE_REG, data & ~0x1);
+		spin_unlock(&iommu->iommu_lock);
+	}
+
+	return count;
+}
+
+static ssize_t sunxi_iommu_profilling_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct sunxi_iommu_dev *iommu = global_iommu_dev;
+	u64 micro_tlb0_access_count;
+	u64 micro_tlb0_hit_count;
+	u64 micro_tlb1_access_count;
+	u64 micro_tlb1_hit_count;
+	u64 micro_tlb2_access_count;
+	u64 micro_tlb2_hit_count;
+	u64 micro_tlb3_access_count;
+	u64 micro_tlb3_hit_count;
+	u64 micro_tlb4_access_count;
+	u64 micro_tlb4_hit_count;
+	u64 micro_tlb5_access_count;
+	u64 micro_tlb5_hit_count;
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+	u64 micro_tlb6_access_count;
+	u64 micro_tlb6_hit_count;
+#endif
+	u64 macrotlb_access_count;
+	u64 macrotlb_hit_count;
+	u64 ptwcache_access_count;
+	u64 ptwcache_hit_count;
+	u64 micro_tlb0_latency;
+	u64 micro_tlb1_latency;
+	u64 micro_tlb2_latency;
+	u64 micro_tlb3_latency;
+	u64 micro_tlb4_latency;
+	u64 micro_tlb5_latency;
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+	u64 micro_tlb6_latency;
+	u32 micro_tlb0_max_latency;
+	u32 micro_tlb1_max_latency;
+	u32 micro_tlb2_max_latency;
+	u32 micro_tlb3_max_latency;
+	u32 micro_tlb4_max_latency;
+	u32 micro_tlb5_max_latency;
+	u32 micro_tlb6_max_latency;
+#endif
+
+	spin_lock(&iommu->iommu_lock);
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+	macrotlb_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG6) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG6);
+	macrotlb_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG6) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG6);
+#else
+	macrotlb_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG7) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG7);
+	macrotlb_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG7) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG7);
+#endif
+
+	micro_tlb0_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG0) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG0);
+	micro_tlb0_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG0) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG0);
+
+	micro_tlb1_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG1) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG1);
+	micro_tlb1_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG1) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG1);
+
+	micro_tlb2_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG2) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG2);
+	micro_tlb2_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG2) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG2);
+
+	micro_tlb3_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG3) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG3);
+	micro_tlb3_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG3) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG3);
+
+	micro_tlb4_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG4) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG4);
+	micro_tlb4_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG4) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG4);
+
+	micro_tlb5_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG5) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG5);
+	micro_tlb5_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG5) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG5);
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+	micro_tlb6_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG6) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG6);
+	micro_tlb6_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG6) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG6);
+
+	ptwcache_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG8) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG8);
+	ptwcache_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG8) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG8);
+#else
+	ptwcache_access_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_HIGH_REG7) &
+		0x7ff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_ACCESS_LOW_REG7);
+	ptwcache_hit_count =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_HIT_HIGH_REG7) &
+		0x7ff) << 32) | sunxi_iommu_read(iommu, IOMMU_PMU_HIT_LOW_REG7);
+#endif
+
+	micro_tlb0_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG0) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG0);
+	micro_tlb1_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG1) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG1);
+	micro_tlb2_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG2) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG2);
+	micro_tlb3_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG3) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG3);
+	micro_tlb4_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG4) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG4);
+	micro_tlb5_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG5) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG5);
+
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+	micro_tlb6_latency =
+		((u64)(sunxi_iommu_read(iommu, IOMMU_PMU_TL_HIGH_REG6) &
+		0x3ffff) << 32) |
+		sunxi_iommu_read(iommu, IOMMU_PMU_TL_LOW_REG6);
+
+	micro_tlb0_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG0);
+	micro_tlb1_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG1);
+	micro_tlb2_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG2);
+	micro_tlb3_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG3);
+	micro_tlb4_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG4);
+	micro_tlb5_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG5);
+	micro_tlb6_max_latency = sunxi_iommu_read(iommu, IOMMU_PMU_ML_REG6);
+
+#endif
+	spin_unlock(&iommu->iommu_lock);
+
+	return snprintf(buf, PAGE_SIZE,
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+		"%s_access_count = 0x%llx\n"
+		"%s_hit_count = 0x%llx\n"
+#endif
+		"macrotlb_access_count = 0x%llx\n"
+		"macrotlb_hit_count = 0x%llx\n"
+		"ptwcache_access_count = 0x%llx\n"
+		"ptwcache_hit_count = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+		"%s_total_latency = 0x%llx\n"
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+		"%s_total_latency = 0x%llx\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+		"%s_max_latency = 0x%x\n"
+#endif
+		,
+		master[0], micro_tlb0_access_count,
+		master[0], micro_tlb0_hit_count,
+		master[1], micro_tlb1_access_count,
+		master[1], micro_tlb1_hit_count,
+		master[2], micro_tlb2_access_count,
+		master[2], micro_tlb2_hit_count,
+		master[3], micro_tlb3_access_count,
+		master[3], micro_tlb3_hit_count,
+		master[4], micro_tlb4_access_count,
+		master[4], micro_tlb4_hit_count,
+		master[5], micro_tlb5_access_count,
+		master[5], micro_tlb5_hit_count,
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+		master[6], micro_tlb6_access_count,
+		master[6], micro_tlb6_hit_count,
+#endif
+		macrotlb_access_count,
+		macrotlb_hit_count,
+		ptwcache_access_count,
+		ptwcache_hit_count,
+		master[0], micro_tlb0_latency,
+		master[1], micro_tlb1_latency,
+		master[2], micro_tlb2_latency,
+		master[3], micro_tlb3_latency,
+		master[4], micro_tlb4_latency,
+		master[5], micro_tlb5_latency
+#if IS_ENABLED(CONFIG_ARCH_SUN8IW15) | IS_ENABLED(CONFIG_ARCH_SUN50IW9) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW19) || IS_ENABLED(CONFIG_ARCH_SUN50IW10) \
+	|| IS_ENABLED(CONFIG_ARCH_SUN8IW20) || IS_ENABLED(CONFIG_ARCH_SUN50IW12)
+		,
+		master[6], micro_tlb6_latency,
+		master[0], micro_tlb0_max_latency,
+		master[1], micro_tlb1_max_latency,
+		master[2], micro_tlb2_max_latency,
+		master[3], micro_tlb3_max_latency,
+		master[4], micro_tlb4_max_latency,
+		master[5], micro_tlb5_max_latency,
+		master[6], micro_tlb6_max_latency
+#endif
+			);
+}
+
+static struct device_attribute sunxi_iommu_enable_attr =
+	__ATTR(enable, 0644, sunxi_iommu_enable_show,
+	sunxi_iommu_enable_store);
+static struct device_attribute sunxi_iommu_profilling_attr =
+	__ATTR(profilling, 0444, sunxi_iommu_profilling_show, NULL);
+
+static void sunxi_iommu_sysfs_create(struct platform_device *_pdev)
+{
+	device_create_file(&_pdev->dev, &sunxi_iommu_enable_attr);
+	device_create_file(&_pdev->dev, &sunxi_iommu_profilling_attr);
+}
+
+static void sunxi_iommu_sysfs_remove(struct platform_device *_pdev)
+{
+	device_remove_file(&_pdev->dev, &sunxi_iommu_enable_attr);
+	device_remove_file(&_pdev->dev, &sunxi_iommu_profilling_attr);
+}
+
+static const struct iommu_ops sunxi_iommu_ops = {
+	.pgsize_bitmap = SZ_4K | SZ_16K | SZ_64K | SZ_256K | SZ_1M | SZ_4M | SZ_16M,
+	.map  = sunxi_iommu_map,
+	.unmap = sunxi_iommu_unmap,
+	.iotlb_sync = sunxi_iommu_iotlb_sync,
+	.domain_alloc = sunxi_iommu_domain_alloc,
+	.domain_free = sunxi_iommu_domain_free,
+	.attach_dev = sunxi_iommu_attach_dev,
+	.detach_dev = sunxi_iommu_detach_dev,
+	.add_device = sunxi_iommu_add_device,
+	.remove_device = sunxi_iommu_remove_device,
+	.device_group	= sunxi_iommu_device_group,
+	.of_xlate = sunxi_iommu_of_xlate,
+	.iova_to_phys = sunxi_iommu_iova_to_phys,
+	.owner = THIS_MODULE,
+};
+
+static int sunxi_iommu_probe(struct platform_device *pdev)
+{
+	int ret, irq;
+	struct device *dev = &pdev->dev;
+	struct sunxi_iommu_dev *sunxi_iommu;
+	struct resource *res;
+
+	iopte_cache = kmem_cache_create("sunxi-iopte-cache", PT_SIZE,
+				PT_SIZE, SLAB_HWCACHE_ALIGN, NULL);
+	if (!iopte_cache) {
+		pr_err("%s: Failed to create sunx-iopte-cache.\n", __func__);
+		return -ENOMEM;
+	}
+
+	sunxi_iommu = devm_kzalloc(dev, sizeof(*sunxi_iommu), GFP_KERNEL);
+	if (!sunxi_iommu)
+		return	-ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_dbg(dev, "Unable to find resource region\n");
+		ret = -ENOENT;
+		goto err_res;
+	}
+
+	sunxi_iommu->base = devm_ioremap_resource(&pdev->dev, res);
+	if (!sunxi_iommu->base) {
+		dev_dbg(dev, "Unable to map IOMEM @ PA:%#x\n",
+				(unsigned int)res->start);
+		ret = -ENOENT;
+		goto err_res;
+	}
+
+	sunxi_iommu->bypass = DEFAULT_BYPASS_VALUE;
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq <= 0) {
+		dev_dbg(dev, "Unable to find IRQ resource\n");
+		ret = -ENOENT;
+		goto err_irq;
+	}
+
+	pr_info("sunxi iommu: irq = %d\n", irq);
+
+	ret = devm_request_irq(dev, irq, sunxi_iommu_irq, 0,
+			dev_name(dev), (void *)sunxi_iommu);
+	if (ret < 0) {
+		dev_dbg(dev, "Unabled to register interrupt handler\n");
+		goto err_irq;
+	}
+
+	sunxi_iommu->irq = irq;
+
+	sunxi_iommu->clk = of_clk_get_by_name(dev->of_node, "iommu");
+	if (IS_ERR(sunxi_iommu->clk)) {
+		sunxi_iommu->clk = NULL;
+		dev_dbg(dev, "Unable to find clock\n");
+		ret = -ENOENT;
+		goto err_clk;
+	}
+	clk_prepare_enable(sunxi_iommu->clk);
+
+	platform_set_drvdata(pdev, sunxi_iommu);
+	sunxi_iommu->dev = dev;
+	spin_lock_init(&sunxi_iommu->iommu_lock);
+	global_iommu_dev = sunxi_iommu;
+
+	if (dev->parent)
+		pm_runtime_enable(dev);
+
+	sunxi_iommu_sysfs_create(pdev);
+
+	ret = iommu_device_sysfs_add(&sunxi_iommu->iommu, dev, NULL,
+				     dev_name(dev));
+	if (ret) {
+		dev_err(dev, "Failed to register iommu in sysfs\n");
+		return ret;
+	}
+
+//	iommu_device_set_ops(&sunxi_iommu->iommu, &sunxi_iommu_ops);
+	sunxi_iommu->iommu.ops = &sunxi_iommu_ops;
+	iommu_device_set_fwnode(&sunxi_iommu->iommu, dev->fwnode);
+
+	ret = iommu_device_register(&sunxi_iommu->iommu);
+	if (ret) {
+		dev_err(dev, "Failed to register iommu\n");
+		return ret;
+	}
+
+	bus_set_iommu(&platform_bus_type, &sunxi_iommu_ops);
+
+	if (!dma_dev)
+		dma_dev = &pdev->dev;
+
+	return 0;
+
+err_clk:
+	free_irq(irq, sunxi_iommu);
+err_irq:
+	devm_iounmap(dev, sunxi_iommu->base);
+err_res:
+	kmem_cache_destroy(iopte_cache);
+	dev_err(dev, "Failed to initialize\n");
+
+	return ret;
+
+}
+
+static int sunxi_iommu_remove(struct platform_device *pdev)
+{
+	struct sunxi_iommu_dev *sunxi_iommu = platform_get_drvdata(pdev);
+
+	kmem_cache_destroy(iopte_cache);
+	bus_set_iommu(&platform_bus_type, NULL);
+	free_irq(sunxi_iommu->irq, sunxi_iommu);
+	devm_iounmap(sunxi_iommu->dev, sunxi_iommu->base);
+	sunxi_iommu_sysfs_remove(pdev);
+	iommu_device_sysfs_remove(&sunxi_iommu->iommu);
+	iommu_device_unregister(&sunxi_iommu->iommu);
+	global_iommu_dev = NULL;
+	return 0;
+}
+
+static int sunxi_iommu_suspend(struct device *dev)
+{
+	clk_disable_unprepare(global_iommu_dev->clk);
+	return 0;
+}
+
+static int sunxi_iommu_resume(struct device *dev)
+{
+	int err;
+
+	clk_prepare_enable(global_iommu_dev->clk);
+
+	if (unlikely(!global_sunxi_iommu_domain))
+		return 0;
+
+	err = sunxi_tlb_init(global_iommu_owner,
+				&global_sunxi_iommu_domain->domain);
+	return err;
+}
+
+const struct dev_pm_ops sunxi_iommu_pm_ops = {
+	.suspend	= sunxi_iommu_suspend,
+	.resume		= sunxi_iommu_resume,
+};
+
+static const struct of_device_id sunxi_iommu_dt[] = {
+	{ .compatible = "allwinner,sunxi-iommu", },
+	{ /* sentinel */ },
+};
+
+MODULE_DEVICE_TABLE(of, sunxi_iommu_dt);
+
+static struct platform_driver sunxi_iommu_driver = {
+	.probe		= sunxi_iommu_probe,
+	.remove		= sunxi_iommu_remove,
+	.driver		= {
+		.owner		= THIS_MODULE,
+		.name		= "sunxi-iommu",
+		.pm 		= &sunxi_iommu_pm_ops,
+		.of_match_table = sunxi_iommu_dt,
+	}
+};
+
+static int __init sunxi_iommu_init(void)
+{
+	return platform_driver_register(&sunxi_iommu_driver);
+}
+
+static void __exit sunxi_iommu_exit(void)
+{
+	return platform_driver_unregister(&sunxi_iommu_driver);
+}
+
+subsys_initcall(sunxi_iommu_init);
+module_exit(sunxi_iommu_exit);
+
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/iommu/sunxi-iommu.h b/drivers/iommu/sunxi-iommu.h
new file mode 100644
index 000000000..fbd350cbc
--- /dev/null
+++ b/drivers/iommu/sunxi-iommu.h
@@ -0,0 +1,476 @@
+/*
+ * sunxi iommu: main structures
+ *
+ * Copyright (C) 2008-2009 Nokia Corporation
+ *
+ * Written by Hiroshi DOYU <Hiroshi.DOYU@nokia.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * Register of IOMMU device
+ */
+#define IOMMU_VERSION_REG				0x0000
+#define IOMMU_RESET_REG					0x0010
+#define IOMMU_ENABLE_REG				0x0020
+#define IOMMU_BYPASS_REG				0x0030
+#define IOMMU_AUTO_GATING_REG			0x0040
+#define IOMMU_WBUF_CTRL_REG				0x0044
+#define IOMMU_OOO_CTRL_REG				0x0048
+#define IOMMU_4KB_BDY_PRT_CTRL_REG		0x004C
+#define IOMMU_TTB_REG					0x0050
+#define IOMMU_TLB_ENABLE_REG			0x0060
+#define IOMMU_TLB_PREFETCH_REG			0x0070
+#define IOMMU_TLB_FLUSH_ENABLE_REG		0x0080
+#define IOMMU_TLB_IVLD_MODE_SEL_REG		0x0084
+#define IOMMU_TLB_IVLD_START_ADDR_REG	0x0088
+#define IOMMU_TLB_IVLD_END_ADDR_REG		0x008C
+#define IOMMU_TLB_IVLD_ADDR_REG			0x0090
+#define IOMMU_TLB_IVLD_ADDR_MASK_REG	0x0094
+#define IOMMU_TLB_IVLD_ENABLE_REG		0x0098
+#define IOMMU_PC_IVLD_MODE_SEL_REG		0x009C
+#define IOMMU_PC_IVLD_ADDR_REG			0x00A0
+#define IOMMU_PC_IVLD_START_ADDR_REG	0x00A4
+#define IOMMU_PC_IVLD_ENABLE_REG		0x00A8
+#define IOMMU_PC_IVLD_END_ADDR_REG		0x00Ac
+#define IOMMU_DM_AUT_CTRL_REG0			0x00B0
+#define IOMMU_DM_AUT_CTRL_REG1			0x00B4
+#define IOMMU_DM_AUT_CTRL_REG2			0x00B8
+#define IOMMU_DM_AUT_CTRL_REG3			0x00BC
+#define IOMMU_DM_AUT_CTRL_REG4			0x00C0
+#define IOMMU_DM_AUT_CTRL_REG5			0x00C4
+#define IOMMU_DM_AUT_CTRL_REG6			0x00C8
+#define IOMMU_DM_AUT_CTRL_REG7			0x00CC
+#define IOMMU_DM_AUT_OVWT_REG			0x00D0
+#define IOMMU_INT_ENABLE_REG			0x0100
+#define IOMMU_INT_CLR_REG				0x0104
+#define IOMMU_INT_STA_REG				0x0108
+#define IOMMU_INT_ERR_ADDR_REG0			0x0110
+
+#define IOMMU_INT_ERR_ADDR_REG1			0x0114 /**/
+#define IOMMU_INT_ERR_ADDR_REG2			0x0118 /**/
+
+#define IOMMU_INT_ERR_ADDR_REG3			0x011C
+#define IOMMU_INT_ERR_ADDR_REG4			0x0120
+#define IOMMU_INT_ERR_ADDR_REG5			0x0124
+
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_INT_ERR_ADDR_REG6			0x0128
+#define IOMMU_INT_ERR_ADDR_REG7			0x0130
+#define IOMMU_INT_ERR_ADDR_REG8			0x0134
+#else
+#define IOMMU_INT_ERR_ADDR_REG6			0x0130
+#define IOMMU_INT_ERR_ADDR_REG7			0x0134
+#endif
+
+#define IOMMU_INT_ERR_DATA_REG0			0x0150
+#define IOMMU_INT_ERR_DATA_REG1			0x0154 /**/
+#define IOMMU_INT_ERR_DATA_REG2			0x0158 /**/
+#define IOMMU_INT_ERR_DATA_REG3			0x015C
+#define IOMMU_INT_ERR_DATA_REG4			0x0160
+#define IOMMU_INT_ERR_DATA_REG5			0x0164
+
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_INT_ERR_DATA_REG6			0x0168
+#define IOMMU_INT_ERR_DATA_REG7			0x0170
+#define IOMMU_INT_ERR_DATA_REG8			0x0174
+#else
+#define IOMMU_INT_ERR_DATA_REG6			0x0170
+#define IOMMU_INT_ERR_DATA_REG7			0x0174
+#endif
+
+#define IOMMU_L1PG_INT_REG				0x0180
+#define IOMMU_L2PG_INT_REG				0x0184
+#define IOMMU_VA_REG					0x0190
+#define IOMMU_VA_DATA_REG				0x0194
+#define IOMMU_VA_CONFIG_REG				0x0198
+#define IOMMU_PMU_ENABLE_REG			0x0200
+#define IOMMU_PMU_CLR_REG				0x0210
+#define IOMMU_PMU_ACCESS_LOW_REG0		0x0230
+#define IOMMU_PMU_ACCESS_HIGH_REG0		0x0234
+#define IOMMU_PMU_HIT_LOW_REG0			0x0238
+#define IOMMU_PMU_HIT_HIGH_REG0			0x023C
+#define IOMMU_PMU_ACCESS_LOW_REG1		0x0240 /**/
+#define IOMMU_PMU_ACCESS_HIGH_REG1		0x0244 /**/
+#define IOMMU_PMU_HIT_LOW_REG1			0x0248 /**/
+#define IOMMU_PMU_HIT_HIGH_REG1			0x024C /**/
+#define IOMMU_PMU_ACCESS_LOW_REG2		0x0250
+#define IOMMU_PMU_ACCESS_HIGH_REG2		0x0254
+#define IOMMU_PMU_HIT_LOW_REG2			0x0258
+#define IOMMU_PMU_HIT_HIGH_REG2			0x025C
+#define IOMMU_PMU_ACCESS_LOW_REG3		0x0260
+#define IOMMU_PMU_ACCESS_HIGH_REG3		0x0264
+#define IOMMU_PMU_HIT_LOW_REG3			0x0268
+#define IOMMU_PMU_HIT_HIGH_REG3			0x026C
+#define IOMMU_PMU_ACCESS_LOW_REG4		0x0270
+#define IOMMU_PMU_ACCESS_HIGH_REG4		0x0274
+#define IOMMU_PMU_HIT_LOW_REG4			0x0278
+#define IOMMU_PMU_HIT_HIGH_REG4			0x027C
+#define IOMMU_PMU_ACCESS_LOW_REG5		0x0280
+#define IOMMU_PMU_ACCESS_HIGH_REG5		0x0284
+#define IOMMU_PMU_HIT_LOW_REG5			0x0288
+#define IOMMU_PMU_HIT_HIGH_REG5			0x028C
+
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ACCESS_LOW_REG6		0x0290
+#define IOMMU_PMU_ACCESS_HIGH_REG6		0x0294
+#define IOMMU_PMU_HIT_LOW_REG6			0x0298
+#define IOMMU_PMU_HIT_HIGH_REG6			0x029C
+#define IOMMU_PMU_ACCESS_LOW_REG7		0x02D0
+#define IOMMU_PMU_ACCESS_HIGH_REG7		0x02D4
+#define IOMMU_PMU_HIT_LOW_REG7			0x02D8
+#define IOMMU_PMU_HIT_HIGH_REG7			0x02DC
+#define IOMMU_PMU_ACCESS_LOW_REG8		0x02E0
+#define IOMMU_PMU_ACCESS_HIGH_REG8		0x02E4
+#define IOMMU_PMU_HIT_LOW_REG8			0x02E8
+#define IOMMU_PMU_HIT_HIGH_REG8			0x02EC
+#else
+#define IOMMU_PMU_ACCESS_LOW_REG6		0x02D0
+#define IOMMU_PMU_ACCESS_HIGH_REG6		0x02D4
+#define IOMMU_PMU_HIT_LOW_REG6			0x02D8
+#define IOMMU_PMU_HIT_HIGH_REG6			0x02DC
+#define IOMMU_PMU_ACCESS_LOW_REG7		0x02E0
+#define IOMMU_PMU_ACCESS_HIGH_REG7		0x02E4
+#define IOMMU_PMU_HIT_LOW_REG7			0x02E8
+#define IOMMU_PMU_HIT_HIGH_REG7			0x02EC
+#endif
+
+
+#define IOMMU_PMU_TL_LOW_REG0			0x0300
+#define IOMMU_PMU_TL_HIGH_REG0			0x0304
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG0				0x0308
+#endif
+
+
+#define IOMMU_PMU_TL_LOW_REG1			0x0310
+#define IOMMU_PMU_TL_HIGH_REG1			0x0314
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG1				0x0318
+#endif
+
+#define IOMMU_PMU_TL_LOW_REG2			0x0320
+#define IOMMU_PMU_TL_HIGH_REG2			0x0324
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG2				0x0328
+#endif
+
+#define IOMMU_PMU_TL_LOW_REG3			0x0330
+#define IOMMU_PMU_TL_HIGH_REG3			0x0334
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG3				0x0338
+#endif
+
+#define IOMMU_PMU_TL_LOW_REG4			0x0340
+#define IOMMU_PMU_TL_HIGH_REG4			0x0344
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG4				0x0348
+#endif
+
+#define IOMMU_PMU_TL_LOW_REG5			0x0350
+#define IOMMU_PMU_TL_HIGH_REG5			0x0354
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG5				0x0358
+#endif
+
+#define IOMMU_PMU_TL_LOW_REG6			0x0360
+#define IOMMU_PMU_TL_HIGH_REG6			0x0364
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define IOMMU_PMU_ML_REG6				0x0368
+#endif
+
+
+
+#define IOMMU_RESET_SHIFT   31
+#define IOMMU_RESET_MASK (1 << IOMMU_RESET_SHIFT)
+#define IOMMU_RESET_SET (0 << 31)
+#define IOMMU_RESET_RELEASE (1 << 31)
+
+/*
+ * IOMMU enable register field
+ */
+#define IOMMU_ENABLE	0x1
+
+/*
+ * IOMMU interrupt id mask
+ */
+#define MICRO_TLB0_INVALID_INTER_MASK   0x1
+#define MICRO_TLB1_INVALID_INTER_MASK   0x2
+#define MICRO_TLB2_INVALID_INTER_MASK   0x4
+#define MICRO_TLB3_INVALID_INTER_MASK   0x8
+#define MICRO_TLB4_INVALID_INTER_MASK   0x10
+#define MICRO_TLB5_INVALID_INTER_MASK   0x20
+#define MICRO_TLB6_INVALID_INTER_MASK   0x40
+
+#define L1_PAGETABLE_INVALID_INTER_MASK   0x10000
+#define L2_PAGETABLE_INVALID_INTER_MASK   0x20000
+
+/*
+ * This version Hardware just only support 4KB page. It have
+ * a two level page table structure, where the first level has
+ * 4096 entries, and the second level has 256 entries. And, the
+ * first level is "Page Directory(PG)", every entry include a
+ * Page Table base address and a few of control bits. Second
+ * level is "Page Table(PT)", every entry include a physical
+ * page address and a few of control bits. Each entry is one
+ * 32-bit word. Most of the bits in the second level entry are
+ * used by hardware.
+ *
+ * Virtual Address Format:
+ *     31              20|19        12|11     0
+ *     +-----------------+------------+--------+
+ *     |    PDE Index    |  PTE Index | offset |
+ *     +-----------------+------------+--------+
+ *
+ * Table Layout:
+ *
+ *      First Level         Second Level
+ *   (Page Directory)       (Page Table)
+ *   ----+---------+0
+ *      |  PDE   |   ---> -+--------+----
+ *    |  ----------+1       |  PTE   |  
+ *    |  |        |         +--------+  |
+ *       ----------+2       |        |  1K
+ *   16K |        |         +--------+  |
+ *       ----------+3       |        |  
+ *    |  |        |         +--------+----
+ *    |  ----------
+ *    |  |        |
+ *      |        |
+ *   ----+--------+
+ *
+ * IOPDE:
+ * 31                     10|9       0
+ * +------------------------+--------+
+ * |   PTE Base Address     |CTRL BIT|
+ * +------------------------+--------+
+ *
+ * IOPTE:
+ * 31                  12|11         0
+ * +---------------------+-----------+
+ * |  Phy Page Address   |  CTRL BIT |
+ * +---------------------+-----------+
+ *
+ */
+
+#define NUM_ENTRIES_PDE 4096
+#define NUM_ENTRIES_PTE 256
+#define PD_SIZE (NUM_ENTRIES_PDE * sizeof(u32))
+#define PT_SIZE	(NUM_ENTRIES_PTE * sizeof(u32))
+
+#define IOMMU_PD_SHIFT 20
+#define IOMMU_PD_MASK  (~((1UL << IOMMU_PD_SHIFT) - 1))
+
+#define IOMMU_PT_SHIFT 12
+#define IOMMU_PT_MASK  (~((1UL << IOMMU_PT_SHIFT) - 1))
+
+#define PAGE_OFFSET_MASK  ((1UL << IOMMU_PT_SHIFT) - 1)
+#define IOPTE_BASE_MASK	 (~(PT_SIZE - 1))
+
+/*
+ * Page Directory Entry Control Bits
+ */
+#define DENT_VALID	0x01
+#define DENT_PTE_SHFIT	10
+#define DENT_WRITABLE	  BIT(3)
+#define DENT_READABLE	  BIT(2)
+
+/*
+ * Page Table Entry Control Bits
+ */
+#define SUNXI_PTE_PAGE_WRITABLE	  BIT(3)
+#define SUNXI_PTE_PAGE_READABLE	  BIT(2)
+#define SUNXI_PTE_PAGE_VALID		  BIT(1)
+
+#define IS_VALID(x)	(((x) & 0x03) == DENT_VALID)
+
+#define IOPDE_INDEX(va)	(((va) >> IOMMU_PD_SHIFT) & (NUM_ENTRIES_PDE - 1))
+#define IOPTE_INDEX(va)	(((va) >> IOMMU_PT_SHIFT) & (NUM_ENTRIES_PTE - 1))
+
+#define IOPTE_BASE(ent) ((ent) & IOPTE_BASE_MASK)
+
+#define IOPTE_TO_PFN(ent) ((*ent) & IOMMU_PT_MASK)
+#define IOVA_PAGE_OFT(va) ((va) & PAGE_OFFSET_MASK)
+
+#define SPAGE_SIZE (1 << IOMMU_PT_SHIFT)
+#define SPD_SIZE (1 << IOMMU_PD_SHIFT)
+#define SPAGE_ALIGN(addr) ALIGN(addr, SPAGE_SIZE)
+#define SPDE_ALIGN(addr) ALIGN(addr, SPD_SIZE)
+#define MAX_SG_SIZE  (128 << 20)
+#define MAX_SG_TABLE_SIZE ((MAX_SG_SIZE / SPAGE_SIZE) * sizeof(u32))
+
+/* IO virtual address start page frame number */
+#define IOVA_START_PFN		(1)
+#define IOVA_PFN(addr)		((addr) >> PAGE_SHIFT)
+#define DMA_32BIT_PFN		IOVA_PFN(DMA_BIT_MASK(32))
+
+/* TLB Invalid ALIGN */
+#define IOVA_4M_ALIGN(iova)	((iova) & (~0x3fffff))
+
+#ifdef CONFIG_ARCH_SUN50IW3
+#define DEFAULT_BYPASS_VALUE     0x3f
+static const u32 master_id_bitmap[] = {0x3, 0x0, 0x0, 0xc, 0x10, 0x20};
+#endif
+#ifdef CONFIG_ARCH_SUN50IW6
+#define DEFAULT_BYPASS_VALUE     0x3f
+static const u32 master_id_bitmap[] = {0x1, 0x0, 0x4, 0xa, 0x10, 0x20};
+#endif
+
+#define SUNXI_PHYS_OFFSET    	0x40000000UL
+#define SUNXI_4G_PHYS_OFFSET    0x100000000UL
+
+/**
+ * sun8iw15p1
+ *	DE :		masterID 0
+ *	E_EDMA:		masterID 1
+ *	E_FE:		masterID 2
+ *	VE:		masterID 3
+ *	CSI:		masterID 4
+ *	G2D:		masterID 5
+ *	E_BE:		masterID 6
+ *
+ * sun50iw9p1:
+ *	DE :		masterID 0
+ *	DI:			masterID 1
+ *	VE_R:		masterID 2
+ *	VE:			masterID 3
+ *	CSI0:		masterID 4
+ *	CSI1:		masterID 5
+ *	G2D:		masterID 6
+ * sun8iw19p1:
+ *	DE :>--->-------masterID 0
+ *	EISE:		masterID 1
+ *	AI:		masterID 2
+ *	VE:>---->-------masterID 3
+ *	CSI:	>-->----masterID 4
+ *	ISP:>-->------	masterID 5
+ *	G2D:>--->-------masterID 6
+ */
+#if defined(CONFIG_ARCH_SUN8IW15) || defined(CONFIG_ARCH_SUN50IW9) \
+	|| defined(CONFIG_ARCH_SUN8IW19) || defined(CONFIG_ARCH_SUN50IW10) \
+	|| defined(CONFIG_ARCH_SUN8IW20) || defined(CONFIG_ARCH_SUN50IW12) \
+	|| defined(CONFIG_ARCH_SUN20IW1)
+#define DEFAULT_BYPASS_VALUE     0x7f
+static const u32 master_id_bitmap[] = {0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40};
+#endif
+
+#define sunxi_wait_when(COND, MS) ({ \
+	unsigned long timeout__ = jiffies + msecs_to_jiffies(MS) + 1;	\
+	int ret__ = 0;							\
+	while ((COND)) {						\
+		if (time_after(jiffies, timeout__)) {			\
+			ret__ = (!COND) ? 0 : -ETIMEDOUT;		\
+			break;						\
+		}							\
+		udelay(1);					\
+	}								\
+	ret__;								\
+})
+
+/*
+ * The format of device tree, and client device how to use it.
+ *
+ * /{
+ *	....
+ *	smmu: iommu@xxxxx {
+ *		compatible = "allwinner,iommu";
+ *		reg = <xxx xxx xxx xxx>;
+ *		interrupts = <GIC_SPI xxx IRQ_TYPE_LEVEL_HIGH>;
+ *		interrupt-names = "iommu-irq";
+ *		clocks = <&iommu_clk>;
+ *		clock-name = "iommu-clk";
+ *		#iommu-cells = <1>;
+ *		status = "enabled";
+ *	};
+ *
+ *	de@xxxxx {
+ *		.....
+ *		iommus = <&smmu ID>;
+ *	};
+ *
+ * }
+ *
+ * Here, ID number is 0 ~ 5, every client device have a unique id.
+ * Every id represent a micro TLB, also represent a master device.
+ *
+ */
+struct sunxi_iommu_dev {
+	struct iommu_device iommu;
+	struct device *dev;
+	void __iomem *base;
+	struct clk *clk;
+	int irq;
+	u32 bypass;
+	spinlock_t iommu_lock;
+};
+
+struct sunxi_iommu_domain {
+	unsigned int *pgtable;		/* first page directory, size is 16KB */
+	u32 *sg_buffer;
+	struct mutex  dt_lock;	/* lock for modifying page table @ pgtable */
+	struct dma_iommu_mapping *mapping;
+	struct iommu_domain domain;
+	//struct iova_domain iovad;
+	/* list of master device, it represent a micro TLB */
+	struct list_head mdevs;
+	spinlock_t lock;
+};
+
+/*
+ * sunxi master device which use iommu.
+ */
+struct sunxi_mdev {
+	struct list_head node;	/* for sunxi_iommu mdevs list */
+	struct device *dev;	/* the master device */
+	unsigned int tlbid;	/* micro TLB id, distinguish device by it */
+	bool flag;
+};
+
+struct sunxi_iommu_owner {
+	unsigned int tlbid;
+	bool flag;
+	struct sunxi_iommu_dev *data;
+	struct device *dev;
+	struct dma_iommu_mapping *mapping;
+};
+
+int sunxi_iova_test_write(dma_addr_t iova, u32 val);
+unsigned long sunxi_iova_test_read(dma_addr_t iova);
+void sunxi_set_debug_mode(void);
+void sunxi_set_prefetch_mode(void);
+extern struct iommu_domain *global_iommu_domain;
+
+
-- 
2.17.1

